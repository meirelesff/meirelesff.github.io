[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ciência de Dados e Aprendizado de Máquina em Ciência Política",
    "section": "",
    "text": "Este é website com materias do curso Ciência de Dados e Aprendizado de Máquina em Ciência Política, ministrado no Programa de Pós-Graduação de Ciência Política da Universidade de São Paulo (USP). Neste espaço serão postados, uma semana antes de cada aula, os materiais de apoio que estudaremos em laboratório.\nA ementa do curso pode ser encontrada aqui."
  },
  {
    "objectID": "index.html#cronograma",
    "href": "index.html#cronograma",
    "title": "Ciência de Dados e Aprendizado de Máquina em Ciência Política",
    "section": "Cronograma",
    "text": "Cronograma\n\n\n\n\n\n\nAviso\n\n\n\nEste é um cronograma inicial sujeito a modificações\n\n\n\n\n\nAula\nData\nTópico\n\n\n\n\n1\n17/08\nApresentação\n\n\n2\n24/08\nIntrodução & Revisão de Programação\n\n\n3\n31/08\nAprendizado Supervisionado: Classificação\n\n\n-\n7/09\nSem aula (Feriado)\n\n\n4\n14/09\nAprendizado Supervisionado: Modelos Lineares\n\n\n-\n21/09\nSem aula (ABCP)\n\n\n5\n28/09\nAprendizado Supervisionado: Text as Data\n\n\n6\n5/10\nAprendizado Supervisionado: Modelos Não-Lineares\n\n\n-\n12/10\nSem aula (Feriado)\n\n\n-\n19/10\nSem aula (Anpocs)\n\n\n7\n26/10\nEnsemble: Stacking, Bagging, Boosting\n\n\n-\n2/11\nSem aula (Feriado)\n\n\n-\n9/11\nSem aula\n\n\n8\n16/11\nAprendizado Não-Supervisionado\n\n\n9\n23/11\nResampling & Validação\n\n\n10\n30/11\nTuning & Feature Selection\n\n\n11\n7/12\nDeep learning\n\n\n12\n14/12\nRevisão de Trabalhos Finais & Encerramento"
  },
  {
    "objectID": "ementa.html",
    "href": "ementa.html",
    "title": "Ementa",
    "section": "",
    "text": "Ementa\n\n\n\nA versão em PDF da ementa pode ser obtida aqui."
  },
  {
    "objectID": "ementa.html#apresentação",
    "href": "ementa.html#apresentação",
    "title": "Ementa",
    "section": "Apresentação",
    "text": "Apresentação\nEste curso introduz um conjunto de ferramentas que nos permitem usar dados de diferentes formatos para responder questões substantivas sobre política. Com ênfase em aprendizado de máquina – i.e., modelos que aprendem a fazer generalizações a partir do reconhecimento de padrões em amostras –, seu objetivo principal é capacitar alunos(as) a aplicar noções de Ciência de Dados e de programação em problemas concretos de classificação, predição e descoberta, o que lhes permitirá construir aplicações como classificadores de texto e de imagem, detectores de outliers ou modelos flexíveis de MrP.\nA abordagem do curso será principalmente prática. Na maior parte do tempo, estudaremos tópicos por meio da resolução de exercícios e da replicação de papers, dentro e fora de sala de aula. De início, após cobrirmos noções úteis de programação de revisarmos a aplicação de modelos de regressão, estudaremos os diferentes tipos de problemas em Ciência de Dados; tipos de aprendizagem e seus principais algoritmos; estratégias de validação e de tuning; e, finalmente, realizaremos projetos que servirão para testar conhecimentos adquiridos. Concluído este percurso, a expectativa é que alunos e alunas obtenham a experiência necessária para incorporar skills de Ciência de Dados em suas rotinas de pesquisa ou de trabalho."
  },
  {
    "objectID": "ementa.html#objetivos",
    "href": "ementa.html#objetivos",
    "title": "Ementa",
    "section": "Objetivos",
    "text": "Objetivos\nSão estes os principais objetivos de ensino do curso:\n\nDesenvolver habilidades de programação. Embora este não seja um curso que ensinará programação diretamente – a como uma forma de aplicar aprendizado de máquina –, alunos e alunas terão a oportunidade de praticar a escrita de código para resolver problemas de pesquisa.\nAprender a conduzir projetos básicos de Ciência de Dados de ponta a ponta. Entre outros, alunos e alunas apreenderão a estruturar perguntas em Ciência de Dados, organizar dados necessários e definir estratégias para respondê-las – o que incluirá criar pipelines, estabelecer métricas de avaliação, validar e ajustar modelos e algoritmos, entre outros.\nEstimular o trabalho colaborativo em pesquisa científica. Por conta da dinâmica do curso, que envolverá trabalhos em duplas e desenvolvimento de papers, alunos e alunas serão desafiados a identificar produções recentes na literatura internacional; e a redigir textos que os(as) ajudem a preparar teses, dissertações ou artigos para publicação."
  },
  {
    "objectID": "ementa.html#pré-requisitos",
    "href": "ementa.html#pré-requisitos",
    "title": "Ementa",
    "section": "Pré-requisitos",
    "text": "Pré-requisitos\nO curso pressupõe conhecimentos de estatística, modelos de regressão e análise de dados. Formalmente, o pré-requisito é já ter cursado a disciplina FLS 6183 Métodos Quantitativos II.\nTambém é esperado que alunos(as) tenham familiaridade com R ou Python. Como escolher entre as duas linguagens? Se você já trabalha com R e seus interesses são acadêmicos, seguir com essa escolha é o melhor; por ser mais demandado no mercado e ser mais usado em áreas conexas, como a engenharia de dados, Python pode ser interessante para quem deseja se qualificar profissionalmente, mas é necessário já ter um nível de programação para além do básico para conseguir acompanhar o curso. Em qualquer caso, recursos didáticos serão disponibilizados em ambas as linguagens, ainda que a minha capacidade de fornecer ajuda seja consideravelmente maior em R."
  },
  {
    "objectID": "ementa.html#leituras",
    "href": "ementa.html#leituras",
    "title": "Ementa",
    "section": "Leituras",
    "text": "Leituras\nEmbora tenhamos poucas leituras analíticas, manuais serão usados para cobrir a implementação de modelos e estudo de conceitos. São eles:\n\n[ITSL] Introduction to statistical learning\n[HML] Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow\n[MLRTM] Machine Learning with R, the tidyverse, and mlr\n[MLR3] MLR3 Book"
  },
  {
    "objectID": "ementa.html#logística",
    "href": "ementa.html#logística",
    "title": "Ementa",
    "section": "Logística",
    "text": "Logística\nNossas aulas terão dois blocos. No inicial e menor deles, teremos uma exposição dos temas abordados e discussão dos trabalhos de leitura indicada. Encerrada esta parte, teremos sessões nas quais alunas e alunos aplicarão conhecimentos vistos. Para facilitar essa parte, usaremos pair programming, prática que consiste no trabalho em duplas no qual há uma parte que principalmente escreve código e, a outra, o revisa e comenta em tempo real."
  },
  {
    "objectID": "ementa.html#avaliação",
    "href": "ementa.html#avaliação",
    "title": "Ementa",
    "section": "Avaliação",
    "text": "Avaliação\nO aproveitamento no curso de cada estudante será avaliado de três formas: exercícios, que serão realizados dentro e fora de sala; dois pequenos projetos; e um working paper final. A nota final no curso será dada pela soma aritmética das notas de cada tarefa avaliativa.\n\nExercícios (15%)\nEm cada aula teremos uma lista de exercícios para praticar o conteúdo visto. Estes serão apresentados em cada aula e deverão ser realizados em duplas seguindo o sistema de pair programming para serem entregues até a aula seguinte. A entrega pontual e regular dos exercícios e o esforço aplicado para resolvê-los serão os critérios de avaliação.\n\n\nProjetos (35%)\nTambém teremos dois pequenos projetos que deverão ser entregues individualmente no formato de notebooks (feitos com Rmarkdown ou Jupyter). A ideia é que ambos os desafios não apenas testem conhecimentos, mas, também, ofereçam ideias de uso de aprendizado de máquina para pesquisas em Ciência Política. Nestes, a avaliação levará em conta a capacidade de aplicar o conhecimento visto ao longo do curso e a capacidade de cumprir os objetivos propostos.\n\nProjeto 1 – Classificador de discursos presidenciais (15%)\nNo primeiro projeto, o objetivo será treinar um modelo de classificação de textos para identificar a autoria de uma amostra de discursos presidenciais no Brasil. Para tanto, será necessário pré-processar textos, transformá-los em bag of words e, finalmente, selecionar modelos mais adequados. Com o trabalho, deverá ser possível derivar probabilidades de um dado discurso ter sido proferido por cada um e cada uma dos presidentes brasileiros incluídos na amostra.\n\n\nProjeto 2 – Classificador de imagens de satélite (20%)\nNo segundo projeto, que corresponderá a 20% da nota final do curso, extraíremos imagens de satélite de locais de votação georreferenciados no Brasil para, usando redes neurais convolucionais, os classificarmos em determinadas categorias. O resultado final deverá ser uma pipeline que permita gerar diferentes esquemas de classificações de imagens de satélite de locais de votação (ou de outras localidades georreferenciadas) no Brasil.\n\n\n\nWorking paper (50%)\nFinalmente, os(as) alunos(as) deverão entregar um working paper a título de avaliação final. Este deverá aplicar algum dos métodos que veremos no curso e ter entre 10 e 15 páginas. Idealmente, será possível aproveitar essa oportunidade para rascunhar um capítulo de tese ou dissertação, ou um artigo para publicação futura. Para estimular o trabalho colaborativo, serão aceitos trabalhos finais realizados em dupla. Criatividade, aplicação correta de noções vistas no curso e estrutura dos textos (i.e., boas motivações, seções adequadas de metodologia e de resultados) serão avaliados.\nNa última aula, alunos e alunas apresentarão suas ideias e resultados parciais para obterem feedback e tirar dúvidas. A datas de entrega da avaliação será combinada no decorrer do curso."
  },
  {
    "objectID": "ementa.html#política-de-gênero",
    "href": "ementa.html#política-de-gênero",
    "title": "Ementa",
    "section": "Política de Gênero",
    "text": "Política de Gênero\nEm aulas de metodologia, homens frequentemente monopolizam a participação. Para evitar isso, seguiremos três protocolos neste curso: no uso de computadores nas atividades de pair programming, mulheres serão priorizadas; para intervir, é necessário estender a mão; quando mulheres falam, colegas não as interrompem."
  },
  {
    "objectID": "ementa.html#atendimento-a-necessidades-especiais",
    "href": "ementa.html#atendimento-a-necessidades-especiais",
    "title": "Ementa",
    "section": "Atendimento a Necessidades Especiais",
    "text": "Atendimento a Necessidades Especiais\nAlunas(os) com quaisquer necessidades ou solicitações individuais não devem exitar em procurar auxílio, tanto por e-mail quanto pessoalmente."
  },
  {
    "objectID": "ementa.html#ferramentas",
    "href": "ementa.html#ferramentas",
    "title": "Ementa",
    "section": "Ferramentas",
    "text": "Ferramentas\nPara resolver tarefas e praticar em casa, certifique-se de ter as ferramentas que usaremos devidamente instaladas em seu computador ou notebook. Para quem usará R, isso inclui tê-lo instalado e, também, o Rstudio. É possível encontrar tutoriais na internet cobrindo os passos necessários. Já para quem pretende usar Python, minha recomendação é usar Python 3 e alguma IDE como VScode ou spyder para escrever e gerenciar scripts e repositórios.\nPara além destes softwares, será necessário instalar algumas das libraries. Em Python, usaremos principalmente o biblioteca Scikit-learn, que oferece um conjunto de ferramentas de pré-processamento, construção de pipelines, seleção e validação de modelos, para além uma ampla gama de algoritmos supervisionados e não-supervisionados; e a biblioteca Keras, que é uma suíte para a construção de modelos de deep learning via Tensorflow. A depender do seu sistema operacional e da disponibilidade de pré-requisitos em seu computador, ambas as bibliotecas podem retornar erros durante a instalação, caso no qual eu posso tentar ajudar em sala ou por e-mail. Também é recomendado utilizar um ambiente virtual antes de fazer qualquer coisa (aqui uma explicação).\nPara instalar os pacotes que precisaremos, basta executar do terminal:\npip install --upgrade pip\npip install tensorflow scikit\nEm R, o equivalente mais próximo do Scikit-learn é a biblioteca mlr3, que também oferece um conjunto de ferramentas e adota princípios de programação orientada a objetos (discutiremos isso em aula). Keras e Tensorflow, por sua vez, já têm versões em R. Para instalar todos os pacotes que usaremos, basta rodar o seguinte código no R:\n\ninstall.packages(c(\"mrl3\", \"tensorflow\", \"keras\"))\n\nIsso feito, é preciso instalar o Tensorflow propriamente dito, o que pode ser feito com:\n\nlibrary(tensorflow)\ninstall_tensorflow()"
  },
  {
    "objectID": "ementa.html#plano-das-aulas",
    "href": "ementa.html#plano-das-aulas",
    "title": "Ementa",
    "section": "Plano das Aulas",
    "text": "Plano das Aulas\n\nAula 1 – Apresentação do curso\n\nLeituras sugeridas:\n\nSalganik (2019)\n\n\n\n\nAula 2 – Introdução à Ciência de Dados & Revisão de Programação\n\nLeituras:\n\nITSL, Cap. 2.1 & 2.2\nHML, Cap. 1\n\nLeituras sugeridas\n\nAthey e Imbens (2019)\nGrimmer, Roberts, e Stewart (2021)\n\n\n\n\nAula 3 – Aprendizado Supervisionado: Classificação\n\nLeituras:\n\nITSL, Cap. 4\n\nLeituras sugeridas:\n\nStreeter (2019)\nMüller (2022)\n\n\n\n\nAula 4 – Aprendizado Supervisionado: Modelos Lineares\n\nLeituras:\n\nITSL, Cap. 3\nHML, Cap. 2 (para Python)\nMLR3, Cap. 2 (para R)\nITSL, Cap. 6\n\nLeituras sugeridas:\n\nLi e Shugart (2016)\nErikson e Wlezien (2021)\n\nAula 5 – Aprendizado Supervisionado: Text as Data\nLeituras:\n\n…\n\nLeituras sugeridas:\n\nBarberá et al. (2021)\nErlich et al. (2021)\n\n\n\n\nAula 6 – Aprendizado Supervisionado: Modelos Não-Lineares\n\nLeituras:\n\nITSL, Cap. 7\n\n\n\n\nAula 7 – Ensemble: Stacking, Bagging, Boosting\n\nLeituras:\n\nITSL, Cap. 8\n\nLeituras sugeridas:\n\nKaufman, Kraft, e Sen (2019)\nMontgomery e Olivella (2018)\nChen e Zhang (2021)\nBroniecki, Leemann, e Wüest (2022)\n\n\n\n\nAula 8 – Aprendizado Não-Supervisionado\n\nLeituras:\n\nITSL, Cap. 12\n\nLeituras sugeridas:\n\nMagyar (2022)\nMueller e Rauh (2018)\nZucco Jr e Power (2021)\n\n\n\n\nAula 9 – Resampling & Validação\n\nLeituras:\n\nITSL, Cap. 5\n\nLeituras sugeridas:\n\nNeunhoeffer e Sternberg (2019)\nRaschka (2018)\n\n\n\n\nAula 10 – Tuning & Feature Selection\n\nLeituras:\n\nMLR3, Cap. 4\n\nLeituras sugeridas:\n\nDenny e Spirling (2018)\nJordan, Paul, e Philips (2022)\n\n\n\n\nAula 11 – Deep learning\n\nITSL, Cap. 10\nHML, Cap. 10.1\nLeituras sugeridas:\n\nChang e Masterson (2020)\nCantú (2019)\nMuchlinski et al. (2021)\n\n\n\n\nAula 12 – Revisão de Trabalhos Finais & Encerramento"
  },
  {
    "objectID": "ementa.html#referências",
    "href": "ementa.html#referências",
    "title": "Ementa",
    "section": "Referências",
    "text": "Referências\n\n\nAthey, Susan, e Guido W Imbens. 2019. «Machine learning methods that economists should know about». Annual Review of Economics 11: 685–725.\n\n\nBarberá, Pablo, Amber E Boydstun, Suzanna Linn, Ryan McMahon, e Jonathan Nagler. 2021. «Automated text classification of news articles: A practical guide». Political Analysis 29 (1): 19–42.\n\n\nBroniecki, Philipp, Lucas Leemann, e Reto Wüest. 2022. «Improved Multilevel Regression with Poststratification through Machine Learning (autoMrP)». The Journal of Politics 84 (1): 000–000.\n\n\nCantú, Francisco. 2019. «The fingerprints of fraud: Evidence from Mexico’s 1988 presidential election». American Political Science Review 113 (3): 710–26.\n\n\nChang, Charles, e Michael Masterson. 2020. «Using word order in political text classification with long short-term memory models». Political Analysis 28 (3): 395–411.\n\n\nChen, Ling, e Hao Zhang. 2021. «Strategic Authoritarianism: The Political Cycles and Selectivity of China’s Tax-Break Policy». American Journal of Political Science 65 (4): 845–61.\n\n\nDenny, Matthew J, e Arthur Spirling. 2018. «Text preprocessing for unsupervised learning: Why it matters, when it misleads, and what to do about it». Political Analysis 26 (2): 168–89.\n\n\nErikson, Robert S, e Christopher Wlezien. 2021. «Forecasting the 2020 presidential election: Leading economic indicators, polls, and the vote». PS: Political Science & Politics 54 (1): 55–58.\n\n\nErlich, Aaron, Stefano G Dantas, Benjamin E Bagozzi, Daniel Berliner, e Brian Palmer-Rubin. 2021. «Multi-label Prediction for Political Text-as-data». Political Analysis, 1–18.\n\n\nGrimmer, Justin, Margaret E Roberts, e Brandon M Stewart. 2021. «Machine learning for social science: An agnostic approach». Annual Review of Political Science 24: 395–419.\n\n\nJordan, Soren, Hannah L Paul, e Andrew Q Philips. 2022. «How to Cautiously Uncover the “Black Box” of Machine Learning Models for Legislative Scholars». Legislative Studies Quarterly.\n\n\nKaufman, Aaron Russell, Peter Kraft, e Maya Sen. 2019. «Improving supreme court forecasting using boosted decision trees». Political Analysis 27 (3): 381–87.\n\n\nLi, Yuhui, e Matthew S Shugart. 2016. «The seat product model of the effective number of parties: A case for applied political science». Electoral Studies 41: 23–34.\n\n\nMagyar, Zsuzsanna B. 2022. «What makes party systems different? A principal component analysis of 17 advanced democracies 1970–2013». Political Analysis 30 (2): 250–68.\n\n\nMontgomery, Jacob M, e Santiago Olivella. 2018. «Tree-Based Models for Political Science Data». American Journal of Political Science 62 (3): 729–44.\n\n\nMuchlinski, David, Xiao Yang, Sarah Birch, Craig Macdonald, e Iadh Ounis. 2021. «We need to go deeper: Measuring electoral violence using convolutional neural networks and social media». Political Science Research and Methods 9 (1): 122–39.\n\n\nMueller, Hannes, e Christopher Rauh. 2018. «Reading between the lines: Prediction of political violence using newspaper text». American Political Science Review 112 (2): 358–75.\n\n\nMüller, Stefan. 2022. «The temporal focus of campaign communication». The Journal of Politics 84 (1): 000–000.\n\n\nNeunhoeffer, Marcel, e Sebastian Sternberg. 2019. «How cross-validation can go wrong and what to do about it». Political Analysis 27 (1): 101–6.\n\n\nRaschka, Sebastian. 2018. «Model evaluation, model selection, and algorithm selection in machine learning». arXiv preprint arXiv:1811.12808.\n\n\nSalganik, Matthew J. 2019. Bit by bit: Social research in the digital age. Princeton University Press. https://www.bitbybitbook.com/en/1st-ed/.\n\n\nStreeter, Shea. 2019. «Lethal force in black and white: Assessing racial disparities in the circumstances of police killings». The Journal of Politics 81 (3): 1124–32.\n\n\nZucco Jr, Cesar, e Timothy J Power. 2021. «Fragmentation without cleavages? Endogenous fractionalization in the Brazilian party system». Comparative Politics 53 (3): 477–500."
  },
  {
    "objectID": "materiais/aula1.html",
    "href": "materiais/aula1.html",
    "title": "Aula 1",
    "section": "",
    "text": "O que é Ciência de Dados? O que são modelos de aprendizado de máquina? Qual a relevância de um curso sobre isso em uma pós-graduação de Ciência Política? Essa aula de apresentação oferecerá algumas respostas a essas perguntas. Além disso, veremos alguns exemplos de aplicação de Ciência de Dados na área das Ciências Sociais, tanto na academia quanto no mercado."
  },
  {
    "objectID": "materiais/aula1.html#primeiras-tarefas",
    "href": "materiais/aula1.html#primeiras-tarefas",
    "title": "Aula 1",
    "section": "Primeiras tarefas",
    "text": "Primeiras tarefas\nNosso trabalho ao longo do curso será muito mais fácil se formos capazes de versionar nossos códigos, isto é, registrar organizadamente mudanças ao longo do tempo. Por conta disso, antes de mais nada será necessário que cada aluno e aluna tenha uma conta no GitHub para usar o git – o mais famoso gerenciador de versões de código open source. Esses são os passos que deverão ser seguidos para tanto:\n\nCrie uma conta no GitHub clicando aqui\nBaixe e instale o git para o seu sistema operacional\nAprenda a\n\nClonar repositórios\nAdicionar e commitar alterações\nSubir alterações e checar status de um repo\n\n\nEm IDEs mais modernas, como Rstudio e VScode, é possível fazer isso de forma simples, sem necessidade abrir o terminal. Aqui há tutoriais cobrindo os dois casos, aqui e aqui.\nPara evitar ter de digitar login e senha repetidamente no git, o ideal é salvar suas credenciais. O modo de se fazer isso variará de acordo com o seu sistema operacional, mas, em geral, digitar isso no seu terminal deve funcionar:\ngit config --global credential.helper store\nEsse método salvará suas credenciais de forma não criptografada, o que não é muito seguro. Se você usa muito git, possui códigos sensíveis e usa Windows, outra alternativa é usar:\ngit config --global credential.helper manager\nO equivalente para Mac OS X é:\ngit config --global credential.helper osxkeychain\n\nBásico de git\n\ngit clone\nA primeira etapa para trabalhar usando versionamento de código é ter um repositório monitorado pelo git. É possível criar um do zero, mas, nesse curso, vamos seguir um caminho mais simples: clonando um repositório criado diretamente no GitHub por vocês (veremos isso em aula).\nO primeiro passo, nesse caso, consiste em achar a página do repositório no GitHub e extrair sua URL .git. O print abaixo mostra como fazer isso – basta clicar em Code, em verde, e copiar a URL que aparece.\n\n\n\n\nURL de um repositório no GitHub\n\n\n\nIsso feito, basta abrir seu terminal, navegar até a pasta onde você deseja salvar o repositório e executar:11 <URL> deve ser trocado pela URL que você copiou na etapa anterior\ngit clone <URL>\n\n\ngit branch\nCada repositório no git tem vários branches, isto é, uma espécie de diretório onde cada novo código adicionado será armazenado. Com o uso de branches, é possível ter diversas versões do mesmo projeto salvas no mesmo repositório, cada uma totalmente diferente da outra caso isso seja útil.\nPor padrão, o branch padrão no GitHub é nomeado main, mas é importante checar isso – vamos precisar usar o nome do branch onde vamos trabalhar pra subir código. Para obter o nome do branch atual, use:\ngit branch\nÉ uma boa prática criar um branch para fazer testes em um código, ou para testar novas funcionalidades. Fazendo isso, qualquer novo código que seja adicionado ficará pendente de revisão no GitHub, o que idealmente deve ser feito por outra pessoa. Aqui há um pequeno texto que trata sobre isso.\n\n\ngit add e git commit\nSuponhamos que você clonou um repositório e adicionou nele um script. Como subir ele para o GitHub? Simples:\ngit add .\nCom isso, toda e qualquer alteração no repositório no seu computador será registrada para ser adicionada ao repositório principal no GitHub. Antes disso, no entanto, é necessário fazer um commit, criar e documentar um snapshot do seu código. Para isso, use:\ngit commit -m \"Meu primeiro arquivo\"\nNote que usamos -m para registrar uma mensagem, algo útil para sabermos o que cada snapshot tem de diferente em relação ao código anterior. É sempre uma boa prática manter esses registros.\n\n\ngit push e git status\nTendo feito alterações e registrado elas com commit, para subir elas para o GitHub basta rodar:\ngit push origin main\nIsso feito, é possível checar o estado atual do repositório com:\ngit status\nO que, se tudo der certo, deve retornar uma mensagem similar a esta abaixo.\n\n\n\ngit status\n\n\n\n\nArquivos básicos\nPor motivos de organização e de armazenamento, há dois arquivos essenciais, que todo repositório no GitHub deve ter: README.md e .gitignore.\nO README.md é um arquivo de texto (em markdown, na verdade) que é exibido na página inicial de um repositório e que serve para documentar seus códigos. Considere sempre criar um e adicione informações úteis, tais como: objetivo do código, o que ele faz, de onde vieram dados, como rodar scripts, entre outros.\nJá o .gitignore serve para registrar alguns arquivos que o git deverá ignorar, ou seja, que o git deixará de fora do versionamento. Isso é útil para bases de dados que, em geral, ocupam bastante espaço de armazenamento e não devem ser hospedadas no GitHub (ele possui um limite de 50mb por arquivo). Prefira sempre armazenar o código que baixa e processa os dados que você precisará, e não uma versão da base de dados já limpa.\n\n\n\nPor que usar versionamento de código?\n\nHave you ever:\n\nMade a change to code, realised it was a mistake and wanted to revert back?\nLost code or had a backup that was too old?\nHad to maintain multiple versions of a product?\nWanted to see the difference between two (or more) versions of your code?\nWanted to prove that a particular change broke or fixed a piece of code?\nWanted to review the history of some code?\nWanted to submit a change to someone else’s code?\nWanted to share your code, or let other people work on your code?\nWanted to see how much work is being done, and where, when and by whom?\nWanted to experiment with a new feature without interfering with working code?\n\nIn these cases, and no doubt others, a version control system should make your life easier.\nTo misquote a friend: A civilised tool for a civilised age.\n\nSabedoria retirada do StackOverflow."
  },
  {
    "objectID": "materiais/aula1.html#materiais-de-apoio",
    "href": "materiais/aula1.html#materiais-de-apoio",
    "title": "Aula 1",
    "section": "Materiais de apoio",
    "text": "Materiais de apoio\n\nTerminal\nÉ muito comum entrar no mundo da Ciência de Dados, ou da programação com R ou Python, sem saber usar o terminal – a famosa telinha preta de onde é possível executar uma série de funções e comandos em bash. Saber usá-lo de forma eficiente, no entanto, é algo útil para automatizar tarefas, instalar dependências necessárias para o funcionamento de alguns softwares e, também, trabalhar com git.\nCaso você precise preencher essa lacuna, estude e pratique o conteúdo desse workshop:\n\nD-Lab Workshop on Bash-Git\n\n\n\nGit e GitHub\nPara praticar ou aprender a usar recursos mais avançados do git e do GitHub, vale consultar:\n\nDocumentação oficial do GitHub\nLearn the Basics of Git in Under 10 Minutes"
  },
  {
    "objectID": "materiais/aula2.html",
    "href": "materiais/aula2.html",
    "title": "Aula 2",
    "section": "",
    "text": "Esta aula cobre as diferenças entre aprendizado de máquia, ou aprendizagem estatística, e outras abordagens de análise de dados. Para quem está habituado a pensar em inferência e propriedades de estimadores, o que veremos é um pouco diferente: nosso objetivo, pelo menos na maioria das nossas aplicações, será predição. A diferença importa e tem a ver, essencialmente, com o tipo de pergunta que poderemos responder.\n\n\nExistem várias definições que poderíamos usar, mas a distinção entre predição e inferência oferica no ITSL é útil. Considere um exemplo hipotético: sabemos quanto candidaturas gastaram em uma eleição, \\(X_{i}\\), e quantos votos obtiveram, \\(Y_{i}\\); com essas informações, poderíamos estimar um modelo como \\(Y_{i} = \\beta X_{i} + \\epsilon_{i}\\). O resultado do modelo estimado nos permitiria, por exemplo, avaliar se o gasto de campanha tem efeito positivo sobre votos, ou para fazer um chute embasado sobre o desempenho de uma candidatura no futuro. No primeiro caso, queremos inferir sobre o processo que deu origem aos dados; no segundo, predizer uma nova ocorrência.\nAlguns exemplos de problemas de inferência:\n\nExaminar se a iluminação pública tem relação com assaltos em São Paulo\nDescobrir quanto um carro desvaloriza com um ano a mais de uso\nSaber se pessoas com ensino superior ganham mais do que as demais\n\nAlguns exemplos de problemas de predição:\n\nA partir de exemplos de um texto de spam, identificar se um texto qualquer é ou não spam\nUsar dados fiscas dos municípios para predizer a votação de uma candidatura específica à reeleição\nOferecer uma sugestão de filme baseada no histórico de usos na Netflix"
  },
  {
    "objectID": "materiais/aula2.html#funções",
    "href": "materiais/aula2.html#funções",
    "title": "Aula 2",
    "section": "Funções",
    "text": "Funções\nCriar funções é algo extremamente útil para se automatizar determinadas rotinas. Neste curso, vamos usar, em alguns casos, funções específicas para facilitar o nosso trabalho. Caso tenha dúvidas, consulte este capítulo do R4DS ou este tutorial do Real Python."
  },
  {
    "objectID": "materiais/aula2.html#controle-de-fluxo",
    "href": "materiais/aula2.html#controle-de-fluxo",
    "title": "Aula 2",
    "section": "Controle de fluxo",
    "text": "Controle de fluxo\nUsar loops é algo normalmente praticado em cursos básicos, mas, para quem tem dúvidas, vale consultar rapidamente essa este capítulo do R4DS, para quem usa R, e este tutorial do Real Python."
  },
  {
    "objectID": "materiais/aula2.html#frameworks",
    "href": "materiais/aula2.html#frameworks",
    "title": "Aula 2",
    "section": "Frameworks",
    "text": "Frameworks\nVamos queimar várias etapas e ir logo para a aplicação dos frameworks. Nas próximas aulas, daremos vários passos atrás e cobriremos detidamente algoritmos e etapas de um projeto.\n\nTarefas\nEm R, será necessário carregar o pacote mle3 e seguir este tutorial. Tente acompanhar o código e ir modificando ele para ver o que acontece (fique à vontade para usar outros modelos ou outros datasets, como o mtcars, que já vem por padrão no pacote datasets).\nEm python, siga o getting started do Sci-kit learn, também brincando com o código. O exemplo, nesse caso, é mais complexo e cobre mais etapas de uma pipeline."
  },
  {
    "objectID": "materiais/aula3.html",
    "href": "materiais/aula3.html",
    "title": "Aula 3",
    "section": "",
    "text": "Programação orientada a objetos\nPara quem usa R, as funções do pacote mlr3 podem parecer um pouco estranhas. Para definir uma tarefa de predição linear, por exemplo, usaríamos o seguinte:\nlibrary(mlr3)\n\nmodelo <- as_task_regr(mpg ~ ., data = mtcars)\nlearner <- lrn(\"regr.lm\")\nlearner$train(modelo)\nEm particular, criamos um objeto learner e depois usamos uma função de dentro dele usando o indexador $ – mais, não salvamos o resultado em lugar algum.\nIsso é possível porque o resultado da função $train() – que é um método do objeto learner – é salvo automaticamento dentro de learner, isto é, ele é salvo in place. Essa é uma das principais características da programação orientada a objetos: objetos têm métodos, isto é, funções internas que, em geral, salvam os resultados de suas chamadas in place.4.4 Nem sempre isso ocorre, mas é uma forma intuitiva de se entender o ponto\nEm Python, isso normalmente é o padrão. Vejamos uma regressão usando sci-kit learn (. é o indexador equivalente de $ nesse contexto):\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nreg = LinearRegression()\nreg.fit(X, y)\nNão é essencial saber sobre classes, métodos e programação orientada a objetos, de forma geral, para acompanhar esse curso, mas pode ser útil aprender um pouco mais. Em R, a principal referência é essa parte do livro Advanced R. Para Python, esse artigo é útil.\n\n\n\n\n\n\nReferências\n\nStreeter, Shea. 2019. «Lethal force in black and white: Assessing racial disparities in the circumstances of police killings». The Journal of Politics 81 (3): 1124–32."
  },
  {
    "objectID": "materiais/aula4.html",
    "href": "materiais/aula4.html",
    "title": "Aula 4",
    "section": "",
    "text": "Nesta aula, aplicaremos os frameworks que começamos a ver mais detidamente para problemas de regressão, isto é, problemas nos quais o objetivo é predizer variáveis contínuas. Também precisaremos definir novas métricas de validação, entender um pouco de regularização e lidar com alguns problemas comuns como o de diferentes distribuições nos nossos preditores.\n\n\nComo deve ser familiar para todo mundo neste curso, modelos de regressão normalmente são apresentados da seguinte forma:\n\\[\nY_{i} = \\alpha + \\beta x_{i} + \\epsilon_{i}\n\\]\nonde \\(Y_{i}\\) indica alguma variável dependente contínua, numérica, como votos obtidos em uma eleição; o preço de um produto; a nota em uma prova. Tipicamente, esse modelo é estimado por OLS e, a partir dos coeficientes como \\(\\beta\\), se fazem inferências. O modelo também tem uma série de pressupostos, mas não nos deteremos nisso aqui – em aprendizado de máquina, isso não é tão relevante porque o objetivo não é inferência.\nDo ponto de vista de predição, OLS minimiza uma métrica simples:\n\\[\n\\sum_{i=1}^{n} (\\hat{y_{i}} - y_{i})^{2}\n\\]\nisto é, a soma das diferenças entre predição e valores reais ao quadrado. Há diferentes formas de se obter o melhor modelo usando OLS e, de forma prática, eles resultam em uma função \\(f(x_{i})\\) que faz novas predições a partir dos coeficientes estimados.\n\n\n\n\n\n\nImportante\n\n\n\nEm regressão, não buscamos acurácia, mas sim minimizar erro\n\n\n\n\n\nProblemas de regressão envolvem usar métricas apropriadas para variáveis contínuas. No geral, queremos saber se um dado modelo faz predições numéricas próximas de determinados valores, caso de um modelo OLS. No entanto, há mais do que isso: em algumas aplicações, também precisaremos obter modelos que reduzam a variação na distanção entre \\(\\hat{Y_i}\\) e \\(Y_{i}\\); em outras, precisaremos lidar com outliers. Para essas e outras tarefas, há métricas de validação mais indicadas.\nPara resumir, segue um quadro com exemplos:\n\nMétricas de validação para problemas de regressão\n\n\n\n\n\n\nMétrica\nIndicação\n\n\n\n\nErro médio (ou MAE)\nDá o mesmo peso para quaisquer erros, isto é, outliers; mesma unidade de \\(Y\\)\n\n\nErro quadrático médio (MSE)\nLeva em conta variação nos erros, penalizando outliers\n\n\nRaiz do erro quadrático médio (RMSE)\nTransforma o MSE trazendo ele de volta para a unidade de \\(Y\\), que pode ser interpretado na escala de desvios-padrão\n\n\n\nExemplos: para predizer o total de vendas diárias de um item baseado em seu preço, o melhor talvez seja usar MAE, uma vez que é possível que valores reais de vendas não tenham variações extremas; ao contrário, para predizer a renda de uma pessoa em função de características demográficas, RMSE ou MSE podem ser mais indicadas. No geral, RMSE é preferido em relação a MSE.\nE o R^2 (ou r-quared)? O R^2, apesar de ser uma métrica de ajuste muito comum em aplicações de inferência por indicar a variação explicada em \\(Y\\) a partir dos preditores, não mensura erro diretamente – o que é mais importante para o caso de unseen data. Isso significa que, em alguns casos, RMSE ou outras podem divergir do R^2, dado que são medidas que buscam mensurar coisas diferentes.\n\n\n\nComo vimos, regressão por OLS encontra sempre o modelo modelo minimizando a soma dos erros quadráticos. Embora essa seja uma métrica razoável, caso queiramos usar outras, ou ainda ajustar o nível de complexidade ou reduzir o peso de variáveis que não ajudam nosso modelo, precisaremos de outras soluções. Para esses e outros casos, regularização é a chave.\nEm termos simples, regularização nada mais é do que um conjunto de métodos utilizados para melhorar o poder preditivo de modelos e/ou evitar overffiting e complexidade. Quando usamos regularização, portanto, estamos mirando em ter modelos melhores e, ao mesmo tempo, menos complexos. Lembrem-se do trade-off entre variânecia e viés: queremos um modelo que evite viéses, isto é, que consiga captar bem sinais evitando ruídos, mas queremos evitar overfitting que pode levar a erros de predição em unseen data.\nHá diversas formas de regularização (veremos isso detidamente em aulas futuras), mas algumas mais comuns incluem cross-validation, que tem a ver com o método de validação utilizado; usar subsets dos preditores; ou penalizar coeficientes (L1, L2 e ElasticNet). Aqui, vamos focar no último cenário, que pode ser implementado com ridge ou LASSO. As variações são simples de se entender:\n\\[\nLASSO = \\sum_{i=1}^{n} (y_{i} - \\sum_{j=1}^{m} x_{ij} \\beta_{j})^{2} + \\lambda \\sum_{j=0}^{m} |\\beta_{j}|\n\\]\n\\[\nRIDGE = \\sum_{i=1}^{n} (y_{i} - \\sum_{j=1}^{m} x_{ij} \\beta_{j})^{2} + \\lambda \\sum_{j=0}^{m} \\beta^{2}_{j}\n\\]\nPopularmente, essas funções são conhecidas por outros nomes:\n\nRegularização por L1, implementada por meio de regressão LASSO, que é útil quando temos muitos preditores e queremos que o modelo encontre as mais úteis (i.e., até mesmo eliminando as menos relevantes)\nRegularização por L2, implementada por meio de regressão Ridge, que é útil quando temos muitos preditores potencialmente importantes (mas queremos reduzir overffiting)\n\nComo deve ser possível perceber, os dois tipos são sensíveis à escala dos preditores – se usamos escalas muito diferentes, especialmente L2, a estimação de \\(\\beta\\) e o valor de \\(\\lambda\\) é impactado. Isso nos leva, portanto, a ter que lidar com outro tipo de problema.\n\n\n\nPara evitar que regularização não funcione adequadamente em problemas de regressão (mas isso também é válido para outros tipos de problemas e para um amplo leque de algoritmos), o ideal é sempre transformas, ou estandarizar, variáveis contínuas para que fiquem em escalas próximas. Bom exemplos incluem:\n\nManter variáveis em proporção, variando de 0 a 1\nUsar z-valores, que nada mais é do que subtrair a média de \\(X\\) para cada unidade \\(i\\) e, depois, dividir cada valor pelo desvio-padrão de \\(X\\)"
  },
  {
    "objectID": "materiais/aula5.html",
    "href": "materiais/aula5.html",
    "title": "Aula 5",
    "section": "",
    "text": "Nesta aula, aplicaremos aprendizado supervisionado para resolver problemas em um dos tipos mais complexos de dado: textos. Em particular, textos são difíceis de serem estudados porque são hiperdimensionais, não-estruturados e, geralmente, volumosos.\nA parte mais importante da aula de hoje consistirá em aprender os principais procedimentos sobre pré-processamento de texto para, então, adaptarmos o workflow que já vimos nas aulas anteriores para classificar discursos presidenciais.\n\n\nComo em qualquer problema em aprendizado de máquina, texto e outras fontes de dados não-estraturadas precisas ser convertidas para números para podermos aplicar algoritmos e pipelines. Há algumas opções mais utilizadas: transformar texto em vetores (word embbedings), algo cada vez mais comum; transformar textos em bag of words (ou bag of n-grams). Bag of words tem a vantagem de ser uma representação simples e que, por resultar em um formato tabular convencional, pode ser utilizado em vários algoritmos.\nResumidamente, bag of words funciona da seguinte forma: listamos todas as (ou um sub-conjunto das) palavras contidas em todos os textos ou documentos que queremos analisar e, para cada texto ou documento, contamos quantas vezes cada uma aparece. O resultado é algo mais ou menos assim:\n\n\n\nBag of Words\n\n\nO grande problema aqui é que, a depender da quantia de textos e dos seus tamanhos, extrapolamos facilmente dezenas de milhares de palavras se formos contabilizar todas – e, ainda assim, a maioria delas vai aparecer em apenas um ou outro documento, criando o que chamamos de matriz esparsa1 Precisamos, portanto, de alternativas.1 Vale notar, também, que bag of words ignora a posição em que palavras ocorrem, o que pode ser importante em algumas aplicações.\n\n\n\nA solução tradicional em aprendizado de máquina para o problema do bag of words é criar matrizes menores fazendo a seleção ativa de apenas algumas palavras para contar. Há várias formas de se fazer isso e, no geral, não há certo ou errado. Para tentar ser mais exaustivo, seguem critérios utizados em pré-processamento de texto:\n\nRemovação de números e/ou caracteres especiais (caso não sejam informativos para determinado contexto);\nRemoção de stopwords, isto é, palavras que não são consideradas informativas para um determinado problema (e.g., artigos, pronomes, etc.);\nNormalização do texto (e.g., lowercase, trimming);\nStemming (também lematização) (e.g., reduzir palavras às suas raízes, como em radical*idade e radical*ização);\nJunção de palavras por similaridade textual (e.g., distância de Jaccard e etc.);\nRemover palavras com mais ou menos que um determinado número de caracteres;\nRemover palavras que ocorrem pouco a) absolutamente, b) relativamente (por proporção, ou quantil); c) por número de documentos únicos em que aparecem;\nRemover palavras indicadoras de contexto (e.g., o nome do autor ou autora de determinado texto);\nEntre outros.\n\nCada uma destas decisões têm impactos diferentes sobre o conjunto final de features utilizados para resolver um problema. De forma geral, no entanto, o importante é sempre mirar no seguinte, via tentativa e erro, para chegar em uma solução adequada: 1) selecionar o mínimo possível de palavras (para sermos eficientes computacionalmente) que discriminem bem valores do nosso target.\n\n\n\n\n\n\nDica\n\n\n\nEm análise de texto, feature selection é algo essencial, diferentemente das aplicações que vimos até agora\n\n\n\n\nEm R, existem alguns frameworks mais bem integrados para análise de texto, mas o próprio mlr3 não oferece suporte específico a essa tarefa. Por essa razão, usaremos uma combinação de duas ferramentas: quanteda e mlr3. Para instalar o primeiro, use:\ninstall.packages(\"quanteda\")\nA partir daí, supondo que temos uma base de textos já carregada chamada df, precisamos seguir três etapas para criar um bag of words: criar um corpus; tokenizar os textos, isto é, separar as palavras; criar um dfm (document-feature matrix) e selecionar as features desejadas:\nlibrary(quanteda)\n\n# 1) Cria um corpus\ncps <- corpus(df, docid_field = \"id\", text_field = \"textos\")\n\n# 2) Tokenizacao\ntks <- cps %>%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(min_nchar = 5, pattern = stopwords(\"pt\"))\n  \n# 3) Criacao de uma matriz bag-of-words\ntks_dfm <- dfm(tks) %>%\n  dfm_trim(min_docfreq = 5)\n  \n# Etapa final) Transformar o resultado para tibble para o mlr3\ntks_dfm <- tks_dfm %>%\n  as.matrix() %>%\n  as_tibble()\nEm Python, esse processo todo é facilitado pela library sklearn. Com ela, já é possível passar uma lista de textos diretamente para o método fit_transform da função CountVectorizer:\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvct = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0.1)\nX = vectorizer.fit_transform(X)\nCountVectorizer não é tão flexível quanto toda a suíte do quanteda, mas Python fornece diversas outras ferramentas para pré-processar textos – inclusive muito mais rápidas.\n\n\n\nQuando criamos uma bag of words para treinar um modelo, precisamos usar as mesmas features que encontramos ali para fazer predição em unseen data. Podemos fazer isso em R ou Python com:\n\nRPython\n\n\n# Tokeniza um corpus com textos nao usados para treino\nteste_tks_dfm <- corpus(teste, docid_field = \"id\", text_field = \"textos\") %>%\n  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%\n  tokens_tolower() %>%\n  tokens_remove(min_nchar = 5, pattern = stopwords(\"pt\")) %>%\n  dfm() %>%\n  dfm_match(featnames(tks_dfm))\n\n\nY = vct.transform(Y)\n\n\n\nFica evidente por esse exemplo como sklearn torna o processo muito mais simples.\n\n\n\nMuitas vezes, bag of words acabam contendo palavras que ocorrem com muita frequência em vários documentos que não são informativas – pense em termos como empresa em documentos corporativos, ou partido em discursos de políticos.\nNestes casos, pode ser útil podenderar o peso das ocorrências de uma palavra pela frequência em que ela aparece em todos os documentos – e aí temos outra medida, TF-IDF.\nA ideia, com isso, é a seguinte: se uma palavra aparece bastante em poucos documentos, provavelmente ela é informativa de alguma similaridade entre eles; ao contrário, se uma palavra aparece pouco ou muito em todos os documentos, possivelmente ela não é informativa.\nPara implementar TF-IDF, podemos usar:\n\nRPython\n\n\ndfm_tfidf(tks_dfm)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvct = TfidfVectorizer()\nX = vectorizer.fit_transform(X)\n\n\n\n\n\n\n\nPor definição, matrizes esparsas contém muitos elementos sem nenhuma informação – e é geralmente isso o que ocorre quando usamos bag of words, especialmente quando são analisados textos muito pequenos. A consequência prática é que muitos algoritmos não lidam bem com estruturas esparsas, falhando em convergir ou em otimizar coeficientes.\nUma boa forma de identificar uma matriz esparsa é contar a proporção de zeros em relação ao total de elementos. Em problemas que usam dados em format ode bag of words, facilmente essa métrica passa de 95%.\nPortanto, quando for escolher algoritmos para lidar com problemas que envolvem texto2, considere opções que lidam bem com matrizes esparsas, como naive bayes, KNN, árvores de decisão ou regressões logísticas (lineares com regularização também são boas opções).2 Vamos estudar apenas classificação, mas é possível gerar features a partir de texto para resolver problemas de regressão.\n\n\n\nPré-processamento é parte integrante de uma pipeline porque, ao alterar features utilizadas – ou seus valores, como quando usamos TF-IDF –, o algoritmo e hiper-parâmetros mais adequado para resolver um dado problema pode mudar. É por isso que, em geral, pré-processamento é otimizado em conjunto com outras decisões.\nPara facilitar algumas coisas, poderemos utilizar algumas ferramentas que nos ajudarão a manejar pipelines um pouco mais complexas. Em R, temos como comparar vários modelos simultaneamente com:\n# Cria um grid para comparar modelos\ndesign <- benchmark_grid(\n  tasks = tsk,\n  learners = lrns(c(\"classif.naive_bayes\", \"classif.kknn\"), predict_sets = \"test\"),\n  resamplings = rsmps(\"holdout\", ratio = 0.7)\n)\n\nresultados <- benchmark(design)\nresultados$score(msr(\"classif.fbeta\"))\nJá em Python, uma ferramenta útil é a função Pipeline:\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import GaussianNB\n\npipe = Pipeline([('texto', CountVectorizer()), ('nb', GaussianNB())])\npipe.fit(X_train, Y_train)\nAlgo importante a notar: a depender do pré-processamento utilizado, como o uso de TF-IDF ou remoção de palavras que ocorrem em poucos documentos, a separação das amostras em treino e teste pode afetar o resultado obtido. Por quê? Se usarmos uma amostra completa para selecionar features, estaremos introduzindo dados que os modelos que serão treinados não deveriam acessar. Algumas pessoas chamam isso de data leaks.\n\n\n\n\n\n\nNota\n\n\n\nPré-processamento deve ser sempre feito antes de introduzirmos uma estratégia de validação"
  },
  {
    "objectID": "materiais/projeto1.html",
    "href": "materiais/projeto1.html",
    "title": "Projeto 1",
    "section": "",
    "text": "Dados\nPara essa tarefa, vocês terão duas bases de dados: uma com os 350 discursos presidenciais e, outra, com 25 discursos sem indicação de autoria que deverá ser utilizada como amostra de validação.\nPara carregar os dados, basta usar:\n\nRPython\n\n\nlink <- \"https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais.csv?raw=true\"\ndiscursos <- readr::read_csv2(link)\n\nlink <- \"https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais_validacao.csv?raw=true\"\nvalidacao <- readr::read_csv2(link)\n\n\nimport pandas as pd\n\nlink = 'https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais.csv?raw=true'\ndiscursos = pd.read_csv(link, sep=';')\n\nlink = 'https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais_validacao.csv?raw=true'\nvalidacao = pd.read_csv(link, sep=';')\n\n\n\n\n\nObjetivo\nO objetivo central dessa atividade é treinar um modelo que performe bem na classificação da autoria dos discursos presidenciais. Para isso, é importante registrar métricas de validação e reportá-las, bem como fazer predições para a amostra de validação que poderemos conferir posteriormente.\n\n\nEntrega\nA entrega deverá ser feita na pasta do GitHub de cada um contendo:\n\nUm notebook ou script com o código utilizado;\nUm documento (pode ser um PDF compilado pelo notebook, mas também pode ser um README.md) detalhando a metodologia utilizada:\n\nModelos testados\nPré-processamento do texto usado\nEstratégia de validação (e.g. ratio no holdout, quantas vezes foi gerada uma nova amostra)\nResumo dos resultados obtidos em alguma métrica de validação (e.g., precisão, recall, F1, etc.)\nPredição para a base de validação com 25 discursos sem indicação de autoria\n\n\nEm aula, poderemos tirar dúvidas sobre o formato da entrega. O importante a notar é que a avaliação não recairá sobre o desempenho dos modelos de vocês na amostra de validação – ela será utilizada apenas para discutirmos os trabalhos de vocês em aula."
  },
  {
    "objectID": "materiais/aula6.html",
    "href": "materiais/aula6.html",
    "title": "Aula 6",
    "section": "",
    "text": "Modelos lineares, ou extensões deles, geralmente são suficientes para conseguirmos bons desempenhos em problemas de classificação e regressão. No mais das vezes, seu uso é o mais justificado também: eles são mais simples e rápidos para treinar e mais fáceis de diagnosticar e interpretar. No entanto, especialmente para problemas nos quais temos muitos preditores com pouca associação com targets, usar modelos não-lineares pode ser uma boa alternativa, ainda que geralmente ao custo de termos mais tempo de treino e menos controle de diagnóstico.\nNesta aula, estudaremos variações simples de modelos lineares, que permitem fazermos recortes arbitrários nas escalas de variáveis – como spinelines – para, então, começarmos a estudar modelos que combinam diferentes variáveis para aumentar o espaço vetorial das nossas features (ainda que frequentemente ao custo de overfitting. Também avançaremos no uso de pipelines e cobriremos brevemente o que são árvores de decisão, um tipo de algoritmo bastante útil para modelar relações não-lineares e interações complexas.\n\n\nOs principais modelos não-lineares são simples extensões de modelos lineares que diferem principalmente em relação à especificação dos modelos, isto é, à transformação das features que usamos. Isso difere, portanto, do que vimos até aqui: para cada aula, em geral vimos algoritmos e modelos diferentes. Agora, o principal é a preparação e seleção de features, o que costuma ser chamado na área de Ciência de Dados de feature engineering.\n\n\nQuando temos uma relação bivariada que não é linear, o mais comum é usarmos preditores elevados em alguma potência, isto é, polinômios:\n\\[\nY_{i} = \\alpha + \\beta_{1} X_{i} + \\beta_{2} X_{i}^{2} ... + \\beta_{n} X_{i}^{n} + \\epsilon_{i}\n\\]\nRecorrendo aos dados climático de São Bernardo do Campo, que já usamos em aulas anteriores, por exemplo, é possível visualizar a diferença do modelo acima, indicado pela curva azul, em relação a um modelo linear, indicado em vermelho.\n\n\nCódigo\nlink <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv\"\ndados <- readr::read_csv(link)\n\nlibrary(tidyverse)\n\ndados %>%\n  ggplot(aes(x = humidity, y = maximum_temprature)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, color = \"steelblue\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4)) +\n  geom_smooth(method = \"lm\", se = F, color = \"orangered3\") +\n  hrbrthemes::theme_ft_rc() +\n  labs(subtitle = \"A linha azul é estimada por um polinômio de ordem 4\")\n\n\n\n\n\nTemperatura máxima vs. humidity\n\n\n\n\nÉ possível notar que o modelo não-linear captura melhor o efeito negativo que é potencializado no final da distribuição. Outra possibilidade é dividir a extensão do preditor em intervalos, bins, e usá-los como variáveis categóricas que estimarão a média dos valores do target:\n\n\nCódigo\ndados %>%\n  mutate(humidity = cut_interval(humidity, 6)) %>%\n  ggplot(aes(x = humidity, y = maximum_temprature)) +\n  geom_point() +\n  geom_boxplot() +\n  hrbrthemes::theme_ft_rc() +\n  labs(subtitle = \"Boxplots indicam intervalos da variável humidity\")\n\n\n\n\n\nTemperatura máxima vs. humidity\n\n\n\n\nOu, ainda, é possível combinar as duas abordagens como em aplicações de spinelines, que cortam as features em faixas e, em casa uma, estima polinômios:\n\n\nCódigo\ndados %>%\n  mutate(humidity2 = cut_interval(humidity, 6)) %>%\n  ggplot(aes(x = humidity, y = maximum_temprature, group = humidity2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F, color = \"orangered3\", formula = y ~ x + I(x^2) + I(x^3) + I(x^4)) +\n  hrbrthemes::theme_ft_rc() +\n  labs(subtitle = \"Polinômios em intervalos da variável humidity\")\n\n\n\n\n\nTemperatura máxima vs. humidity\n\n\n\n\n\n\n\n\nHá alguns algoritmos que implementam, de forma automática, o pré-processamento de variáveis para criar polinômios, bins e interações entre variáveis. No sklearn, por exemplo, existem funções de pré-processamento de polinômios como sklearn.preprocessing.PolynomialFeatures, assim como no mlr3 há o quantilebins, que divide variáveis em espaços categóricos.\nTambém há modelos que trabalham para modelar relações complexas, não-lineares, de forma automatizada. O principal deles é o Multivariate Adaptive Regression Splines (MARS), que automaticamente encontra a melhor forma de criar bins (ou knots) para estimar \\(N\\) modelos lineares. Isso é feito de forma iterativa, o que significa que o modelo minimiza uma função para selecionar melhores knots (em certo sentido, MARS é um tipo de ensemble, que veremos detidamente na próxima aula). Visualmente, o resultado do MARS é mais ou menos esse (em um caso bivariado):\n\n\n\n\nExemplo do MARS\n\n\n\nEm R, é possível implementar o MARS com o algoritmo regr.earth no pacote mlr3. Em Python, é necessário instalar a lib py-earth com sudo pip install sklearn-contrib-py-earth e usar from pyearth import Earth normalmente, como qualquer outro modelo do sklearn.11 É possível que a instalação do Earth dê problemas variados por conta de dependências.\n\n\n\nComo vimos na última aula, pré-processamento é essencial, mas geralmente toma muitas linhas de código, problema que é ainda mais agudo quando queremos comparar o desempenho de diferentes modelos – lembrando que é necessário pré-processar amostras de treino e teste separadamente quando usamos processamentos que introduzem modificações relacionais, que dependem de parâmetros da base como um todo.\nÉ por conta disso que a maioria dos frameworks de aprendizado de máquina incluem funções que facilitam as tarefas de pré-processamento e comparação de modelos. No sklearn, como vimos na última aula, temos a Pipeline que serve para isso e, no mlr3, temos grafos implementados por meio da função po e dos pipe operators.\nNo exemplo abaixo, implementamos uma pipeline para estandarizar variáveis contínuas para, então, treinar modelos lineares:\n\nRPython\n\n\n# Carrega pacotes\nlibrary(tidyverse)\nlibrary(mlr3verse)\n\n# Carrega dados\nlink <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv\"\ndados <- readr::read_csv(link) %>%\n  select_if(is.numeric)\n\n# Define a task\ntsk <- as_task_regr(maximum_temprature ~ ., data = dados)\n\n# Cria uma pipeline simples\ngr <- po(\"scale\") %>>%\n  po(\"learner\", learner = lrn(\"regr.lm\")) %>%\n  as_learner()\n  \n# Treina a pipeline com 'benchmark_grid' e calcula metrica de validacao\ndesign <- benchmark_grid(\n  tasks = tsk,\n  learners = list(gr),\n  resamplings = rsmp(\"holdout\", ratio = 0.7)\n)\n\nresultados <- benchmark(design)\nresultados$score(msr(\"regr.rmse\"))\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\n\n# Carrega dados e separa as amoastras\nlink = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv'\ndados = pd.read_csv(link).select_dtypes(['number'])\n\nY = dados['maximum_temprature']\nX = dados.loc[:, dados.columns != 'maximum_temprature']\n\nX_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y)\n\n# Cria uma pipeline\npipe = Pipeline([('scale', StandardScaler()), ('lm', LinearRegression())])\n\n# Treina o modelo, calcula o RMSE\npipe.fit(X_treino, Y_treino)\nmean_squared_error(Y_teste, pipe.predict(X_teste))\n\n# Treina o modelo na base toda\npipe.fit(X, Y)\n\n\n\nCom esse novo workflow, que é bem próximo do que usamos em aplicações reais (exceto pelo fato de ainda não usarmos estratégias melhores de resampling, de tuning e de pré-processamento), fica fácil encontrar o melhor modelo, treiná-lo na base completa e validá-lo em outra base. Por exemplo, podemos aplicar o novo modelo para predizer a maximum_temprature de Campinas:\n\nRPython\n\n\n# Usa uma amostra de validacao diferente\nvalidacao <- read_csv(\"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/master/web%20scraping/Cleaned%20Data/Brazil_Campinas_Cleaned.csv\")\n\nmodelo <- gr$train(tsk)\npred <- modelo$predict_newdata(validacao)\nvalidacao$pred <- pred$response\npred$score(msr(\"classif.fbeta\"))\n\n\n# Carrega dados de validacao\nlink = '\"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/master/web%20scraping/Cleaned%20Data/Brazil_Campinas_Cleaned.csv\"'\nvalidacao = pd.read_csv(link).select_dtypes(['number'])\n\nY_val = validacao['maximum_temprature']\nX_val = validacao.loc[:, validacao.columns != 'maximum_temprature']\n\n# Faz predicoes na base de validacao\npred = pipe.predict(X_val)\nmean_squared_error(Y_val, pred)\n\n\n\nTambém conseguimos facilmente produzir polinômios:\n\nRPython\n\n\n# Cria uma pipeline com polinomios\ngr2 <- po(\"scale\") %>>%\n  po(\"mutate\") %>>%\n  po(\"learner\", learner = lrn(\"regr.lm\")) %>%\n  as_learner()\n\ngr2$param_set$values$mutate.mutation <- list(\n  teste = ~ cloud_coverage^2 + cloud_coverage^3,\n  teste2 = ~ pressure^2 + pressure^3)\n\n\n# Cria uma pipeline com polinomios\nfrom sklearn.preprocessing import PolynomialFeatures\n\npipe = Pipeline([('scale', StandardScaler()), ('poly', PolynomialFeatures()), ('lm', LinearRegression())])\n\n\n\n\n\n\nÁrvores de decisão partem de um princípio muito simples: elas particionam as features em combinações, ou regiões, e, com base nisso, fazem predições. Tomando a base de passageiros do Titanic (um dataset popular na Ciência de Dados), isso pode ser representado assim:\n\n\n\nRepresentação de uma árvore de decisão\n\n\nEm árvores de decisão, variáveis contínuas são transformadas em bins e combinadas com categoricas a partir de decision rules, ou split rules, que comandam como o algoritmo agrupará variáveis em regiões – pelo diagrama acima, dá para entender o nome do algoritmo. Por conta dessa estrutura flexível, não-paramétrica, árvores conseguem aprender funções mesmo quando estas envolvem interações complexas e não-lineares nos dados, mas isso geralmente vem associado a um maior overfitting e consequente aumento de variação (na próxima aula, veremos como lidar com isso).\nHá diferentes algoritmos que implementam árvores de decisão, bem como diferentes regras de split e hiperparâmetros que configuram essas regras, mas, por hoje, nos deteremos em implementar versões simples: CART, de classification and regression trees, que está disponível no sklearn como sklearn.tree e, no mlr3, como rpart22 Os dois algoritmos, na verdade, são adaptações do algoritmo original. Além deles, há inúmeras outras variações."
  },
  {
    "objectID": "materiais/aula7.html",
    "href": "materiais/aula7.html",
    "title": "Aula 7",
    "section": "",
    "text": "Até agora no curso, usamos abordagens simples de aprendizagem supervisionada: treinamos um ou mais modelos, os validamos e, do melhor deles, derivamos predições. Nesta aula, veremos uma abordagem mais sofisticada: combinar diferentes modelos, o que é chamado de ensemble em aprendizado de máquina.\nNesta aula, estudaremos as três formas mais comuns de combinação de modelos – bagging, stacking e boosting – e veremos em quais situações cada uma é mais indicada. Abaixo, um quadro com um resumo das abordagens que estudaremos:\n\nMétodos de ensemble mais comuns\n\n\n\n\n\n\nEnsemble\nDescriçao\n\n\n\n\nBagging\nTreina-se \\(N\\) modelos em variaçoes da amostra (bootstrap, subsamples, feature bagging, etc.)\n\n\nStacking\nTreina-se \\(N\\) modelos diferentes, combinados por meio de um segundo modelo (ou blender)\n\n\nBoosting\nTreina-se \\(N\\) modelos sequencialmente, cada um corrigindo os erros do anterior"
  },
  {
    "objectID": "materiais/aula7.html#bagging",
    "href": "materiais/aula7.html#bagging",
    "title": "Aula 7",
    "section": "Bagging",
    "text": "Bagging\nCombinar modelos para melhorar predições é algo que, apesar fazer sentido intuitivamente, não é facilmente implementável. Como combinar modelos? Fazemos uma média de diferentes modelos? O quão diferentes precisam ser os modelos? Há uma variedade de respostas a essas perguntas, mas vale começarmos pelo exemplo que exploramos na última aula: usar árvores de decisão para predizer a temperatura máxima diária em São Bernardo do Campo.\nComo vimos, uma árvore de decisão para esse problema poderia fazer predições usando este diagrama:\n\n\n\n\nflowchart LR\n   A(Maximum temprature) --> B{Humidity > 0.3}\n   B -->|Não| H{< 15}\n   B -->|Sim| C{Preassure > 0.5} -->|Sim| G(< 20)\n   C -->|Não| D{> 20}\n\n\n\n\n\nÁrvore hipotética\n\n\n\nAgora imagine que customizamos a regra de splits do modelo e produzimos outra árvore, assim:\n\n\n\n\nflowchart LR\n   A(Maximum temprature) --> B{Preassure > 0.5}\n   B -->|Não| H{> 15}\n   B -->|Sim| C{< 15} \n\n\n\n\n\nÁrvore hipotética II\n\n\n\nÁrvores de decisão são boas opções para evitar viés porque, de forma não-paramétrica, encontram relações não-lineares e interações profundas – mas isso ao custo de frequentemente termos overfitting e alta variância, isto é, diferentes árvores treinadas em diferentes amostras podem resultar em modelos muito diferentes que não generalizam bem para unseen data. O exemplo acima, embora fictício, ilustra o problema.\nMas e se, em vez de treinarmos uma única árvore de decisão, treinarmos \\(N\\), com diferentes variações, e combinarmos seus resultados tirando uma média de suas predições? Resumidamente, esta é a ideia central por detrás do bagging. Usando bootstrap, sorteamos \\(N\\) observações de uma amostra, onde \\(N\\) é o número de observações na mesma amostra, com substituição e treinamos um árvore para cada. Ao final, usamos cada árvore para fazer predições na amostra de teste e as agregamos por média (caso de problemas de regressão) ou por frequência (caso de problemas de classificação) – averaging e voting, como essas formas de agregação são conhecidas, respectivamente.\nHá variações na implementação do bagging. É possível, por exemplo, usar bootstrap puro, mas também sortear apenas um percentual das observações de uma amostra, o que é chamado de subsampling1; para evitar que apenas variáveis importantes pesem na construção de árvores (em geral, estas ocupam o primeiro nó nas árvores), também é comum sortear um sub-conjunto de features para cada split, o que é chamado de feature bagging ou feature sampling. Por ser flexível, o método permite diferentes customizações.1 Algo útil é usar observações que ficaram de fora de cada sorteio, chamadas de out-of-bag, para calcular métricas de valiação; no sklearn, isso é possível diretamente por meio de um dos argumentos de BaggingRegressor ou BaggingClassifier.\nA vantagem principal do bagging é reduzir a variância em predições. Como árvores diferentes, treinadas em partes diferentes de uma amostra, podem captar diferentes padrões ao custo de overfitting, combinar vários significa dar peso a predições que ocorrem na maioria das árvores treinadas. Como resume o ITSL:\n\nBagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure. ITSL, p. 341\n\nO resultado disso geralmente é bem visível: melhora em métricas de validação, como podemos ver em um exemplo simples de regressão, usando a nossa já conhecida base de dados climáticos de São Bernardo do Campo:\n\nRPython\n\n\n# Carrega pacotes\nlibrary(tidyverse)\nlibrary(mlr3verse)\n\n# Carrega dados\nlink <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv\"\ndados <- readr::read_csv(link) %>%\n  select_if(is.numeric)\n\n# Define a task\ntsk <- as_task_regr(maximum_temprature ~ ., data = dados)\n\n# Cria uma pipeline com arvore de decisao\ngr <- po(\"scale\") %>>%\n  po(\"learner\", learner = lrn(\"regr.rpart\")) %>%\n  as_learner()\n\n# Cria uma pipeline com bagging (sumsample + bootstrap)\ngr_bagging <- po(\"subsample\", frac = 1, replace = TRUE) %>>%\n  po(\"learner\", learner = lrn(\"regr.rpart\")) %>%\n  ppl(\"greplicate\", ., 10) %>>%\n  po(\"regravg\", innum = 10) %>%\n  as_learner()\n\n# Treina as pipelines\ndesign <- benchmark_grid(\n  tasks = tsk,\n  learners = list(gr, gr_bagging),\n  resamplings = rsmp(\"holdout\", ratio = 0.7)\n)\n\nresultados <- benchmark(design)\nresultados$score(msr(\"regr.rmse\"))\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import BaggingRegressor\nimport pandas as pd\n\n# Carrega dados e separa as amostras\nlink = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv'\ndados = pd.read_csv(link).select_dtypes(['number'])\n\nY = dados['maximum_temprature']\nX = dados.loc[:, dados.columns != 'maximum_temprature']\n\nX_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y)\n\n# Treina os modelos\narvore = DecisionTreeRegressor().fit(X_treino, Y_treino)\nbagging = BaggingRegressor(\n    base_estimator=DecisionTreeRegressor(),\n    n_estimators=10,\n).fit(X_treino, Y_treino)\n\n# Calcula metricas\npred_arvore = arvore.predict(X_teste)\nmean_squared_error(Y_teste, pred_arvore)\n\npred_bagging = bagging.predict(X_teste)\nmean_squared_error(Y_teste, pred_bagging, squared=False)\n\n\n\nAs grandes novidades aqui são que, em R, usamos po(\"subsample\", frac = 1, replace = TRUE) para criar bootstraps na amostra, ppl(\"greplicate\", ., 10) para replicar o procedimento 10 vezes e, por fim, po(\"regravg\", innum = 10) para agregar diferentes predições. Em Python, o processo é basicamente o mesmo – uma das vantagens do framework ser mais maduro é a consistência de código –, mas usamos BaggingRegressor, que permite usar bootstrap, subsample e feature bagging.\nComo é possível checar na documentação dos frameworks que usamos, bagging permite muitas adaptações: é possível usar diferentes modelos (não apenas árvores), com diferentes hiper-parâmetros e estratégias de resampling as mais variadas.\n\nRandom forest\nPela sua popularidade, vale reservar um espaço para detalhar um dos algoritmos de bagging: random forest. Como o nome sugere, elas nada mais são do que árvores, combinadas por meio de ensemble, que são treinadas em variações da amostra (com subsample) e variações de features (feature bagging) simultaneamente. Assim, acabamos com árvores que se especializam em predizer determinadas partes da amostra utilizando diferentes preditores – o que evita que uma ou poucas variáveis acabem resultado em modelos similares, isto é, que tenhamos árvores correlacionadas. Nesse sentido, random forest incentiva a produção de modelos os mais diferentes2 para, depois, combiná-los.2 Na verdade, há variações do algoritmo que geral splits de forma totalmente aleatória (extreme random forest, por exemplo), mas em geral performam pior.\nPodemos implementar random forest da seguinte forma:\n\nRPython\n\n\n# install.packages(\"randomForest\")\nlibrary(mlr3extralearners)\n\ngr <- po(\"learner\", learner = lrn(\"regr.randomForest\", ntree = 50)) %>%\n  as_learner()\n\ndesign <- benchmark_grid(\n  tasks = tsk,\n  learners = list(gr),\n  resamplings = rsmp(\"holdout\", ratio = 0.7)\n)\n\nresultados <- benchmark(design)\nresultados$score(msr(\"regr.rmse\"))\n\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=50)\\\n    .fit(X_treino, Y_treino)\n\npred = rf.predict(X_teste)\nmean_squared_error(Y_teste, pred, squared=False)\n\n\n\n\n\n\n\n\n\nNúmero de árvores\n\n\n\nAo usar bagging com árvores (o mais comum), considere sempre treinar o maior número de árvores possível\n\n\n\n\n\n\n\n\nEstandardização\n\n\n\nPor usar split rules e permitir combinações complexas e não-aleatórias, árvores de decisão geralmente não requerem normalização de variáveis contínuas"
  },
  {
    "objectID": "materiais/aula7.html#stacking",
    "href": "materiais/aula7.html#stacking",
    "title": "Aula 7",
    "section": "Stacking",
    "text": "Stacking\nComo vimos, bagging agrega predições de diferentes modelos – não apenas árvores, emboras bagged trees seja a aplicação mais comum deste ensemble. Em stacking, fazemos a mesma coisa, usando basicamente o mesmo procedimento, com a exceção de que empregamos um modelo para agregar predições feitas pelos modelos treinados no primeiro estágio. Em vez de calcular uma média ou a moda, portanto, stacking treina um novo modelo para mapear predições dos seus modelos constitutivos.\nDo ponto de vista de implementação, stacking funciona da seguinte forma3: separamos três amostras, 2 de treino e uma de teste; treinamos \\(N\\) modelos de nível 0 na primeira amostra de treino; fazemos predições usando esses \\(N\\) modelos para a segunda base de treino; treinamos um modelo de nível 1 para agregar as predições do primeiro estágio; finalmente, fazemos predições para a base de teste e calculamos métricas de validação. Visualmente, isso resulta no seguinte workflow:3 Há variações nessa implementação; com sklearn, por exemplo, o padrão é se usar Kfold cross-validation, algo que veremos em aulas posteriores, para gerar predições para todas as observações na amostra de treino sem muitos leaks. A ideia que está descrita aqui é mais próxima do conceito de blending, popularizada por uma competição criada pela Netflix para melhorar seus algoritmos de recomendação.\n\n\n\n\nImagem: javapoint\n\n\n\nTradicionalmente, stacking é usado com algoritmos diferentes, às vezes com hiper-parâmetros diferentes, de forma a fazer com que tenham baixa correlação entre si, isto é, que suas predições sejam intencionalmente muito diferentes – o que dá variação suficiente que pode ser útil para criar um ensemble mais complexo. Além disso, é comum usar um blender simples, como regressão linear ou regressão linear com regularização, o que ajuda a dar menor peso para modelos nível 0 que não contribuem muito com a performance do stack ensemble.\nA título de exemplo, podemos montar um ensemble stacking que usa regressão linear simples, KNN e random forest4 para predizer a temparatura máxima em São Bernardo do Campo:4 Como dá para ver, é possível usar um ensemble dentro de outro\n\nRPython\n\n\n# Cria modelos que retornam predicoes out-of-sample (\"learner_cv\")\ngr <- po(\"scale\") %>>%\n  po(\"learner\", learner = lrn(\"regr.lm\")) %>%\n  po(\"learner_cv\", .) \n\ngr_kknn <- po(\"scale\") %>>%\n  po(\"learner\", learner = lrn(\"regr.kknn\")) %>%\n  po(\"learner_cv\", .) \n\ngr_rf <- po(\"learner\", learner = lrn(\"regr.randomForest\", ntree = 50)) %>%\n  po(\"learner_cv\", .) \n\n# Cria o ensemble\nstack <- list(gr, gr_kknn, gr_rf) %>%\n  gunion() %>>% # Une os modelos\n  po(\"featureunion\") %>>% # Une as predicoes\n  po(\"learner\", learner = lrn(\"regr.lm\")) %>% # Faz predicoes finais\n  as_learner()\n  \ndesign <- benchmark_grid(\n  tasks = tsk,\n  learners = list(stack),\n  resamplings = rsmp(\"holdout\", ratio = 0.7)\n)\n\nresultados <- benchmark(design)\nresultados$score(msr(\"regr.rmse\"))\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Treina os modelos\nmodelos = [('lm', LinearRegression()),\n           ('kknn', KNeighborsRegressor()),\n           ('rf', RandomForestRegressor(n_estimators=50))\n          ]\n\nstack = StackingRegressor(estimators=modelos, final_estimator=LinearRegression())\\\n    .fit(X_treino, Y_treino)\n\n# Calcula metricas\npred = stack.predict(X_teste)\nmean_squared_error(Y_teste, pred, squared=False)\n\n\n\nEm R, como dá para notar, o processo é bem mais complexo do que em Python, razão pela qual pode ser útil consultar a seção específica sobre isso no livro do framework.\nVale dizer também que o ITSL não cobre stacking – a bem da verdade, poucos livros dedicam muito espaço a ele –, mas é algo útil por ser uma ferramenta extremamente flexível e, também, por ser uma versão similar de outros métodos de ensemble mais avançados, como BMA (Bayesian Model Averaging), que não cobriremos neste curso."
  },
  {
    "objectID": "materiais/aula7.html#boosting",
    "href": "materiais/aula7.html#boosting",
    "title": "Aula 7",
    "section": "Boosting",
    "text": "Boosting\nDiferentemente de bagging e stacking, que treinam modelos fracos (weak learners) e combinam suas predições, boosting treina modelos fracos de forma sequencial, com cada novo modelo aprendendo a corrigir os principais erros feitos pelo anterior. Assim, ao final da fase de treino dos \\(N\\) modelos o que temos é um conjunto ordenado de modelos que fazem predições voltadas a reduzir diferentes tipos de erro – consequentemente, minimizando viés e, em certa medida, também variância.\nEm sua versão mais conhecida, chamada de gradient boosting, o processo de treino do ensemble repete algumas vezes o seguinte fluxo: treina-se um modelo qualquer nos dados (árvore de decisão simples é o mais comum), faz-se predição na amostra de treino e calcula-se os resíduos; usando os resíduos como target, treina-se nova árvore. Repetindo esse procedimento algumas vezes, basta somar as predições de cada modelo individual na amostra de teste para se obter uma predição final5. O diagrama a seguir mostra um exemplo do processo:5 Há outra versão popular de boosting, que é implementada pelo conhecido AdaBoost, que em vez de usar os resíduos dos modelos usa pesos na estimação de novas predições.\n\n\n\n\nImagem: Andreas Muller\n\n\n\nAssim como no caso do random forest, boosting geralmente é implementado por um algoritmo sem maior necessidade de adaptação de código. Podemos estimar modelos com boosting usando extreme gradient boosting, uma das versões mais rápidas e otimizadas para predição, e implementações comuns de gradient boosting usando:\n\nRPython\n\n\ngr_xgboost <- po(\"learner\", learner = lrn(\"regr.xgboost\", nrounds = 50)) %>%\n  as_learner()\n\ngr_gbm <- po(\"learner\", learner = lrn(\"regr.gbm\", n.trees = 50)) %>%\n  as_learner()\n\ndesign <- benchmark_grid(\n  tasks = tsk,\n  learners = list(gr_xgboost, gr_gbm),\n  resamplings = rsmp(\"holdout\", ratio = 0.7)\n)\n\nresultados <- benchmark(design)\nresultados$score(msr(\"regr.rmse\"))\n\n\n# Necessário instalar o xgboost, que não\n# faz parte do sklearn\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Treina os modelos\ngb = GradientBoostingRegressor(n_estimators=50)\\\n    .fit(X_treino, Y_treino)\n\nxgb = XGBRegressor(n_estimators=50)\\\n    .fit(X_treino, Y_treino)\n\n# Calcula metricas\npred = gb.predict(X_teste)\nmean_squared_error(Y_teste, pred, squared=False)\n\npred = xgb.predict(X_teste)\nmean_squared_error(Y_teste, pred, squared=False)"
  },
  {
    "objectID": "materiais/aula7.html#outros-tipos-de-ensemble",
    "href": "materiais/aula7.html#outros-tipos-de-ensemble",
    "title": "Aula 7",
    "section": "Outros tipos de ensemble",
    "text": "Outros tipos de ensemble\nEmbora bagging, stacking e boosting sejam, de longe, os tipos mais utilizados de emsemble, existem diversas outras maneiras de se combinar modelos. Um bom material de referência para o Bayesian Averaging, uma aplicação que cresceu na Ciência Política, pode ser vista aqui."
  },
  {
    "objectID": "materiais/aula8.html",
    "href": "materiais/aula8.html",
    "title": "Aula 8",
    "section": "",
    "text": "Referências\n\nJames, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2013. An introduction to statistical learning. Vol. 112. Springer."
  },
  {
    "objectID": "materiais/aula9.html",
    "href": "materiais/aula9.html",
    "title": "Aula 9",
    "section": "",
    "text": "Referências\n\nJames, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2013. An introduction to statistical learning. Vol. 112. Springer.\n\n\nNeunhoeffer, Marcel, e Sebastian Sternberg. 2019. «How cross-validation can go wrong and what to do about it». Political Analysis 27 (1): 101–6."
  },
  {
    "objectID": "materiais/aula10.html",
    "href": "materiais/aula10.html",
    "title": "Aula 10",
    "section": "",
    "text": "Qualquer problema supervisionado, como vimos ao longe deste curso, está sujeito ao trade-off entre viés e variância: um modelo que aprende a reduzir o viés na estimação de parâmetros a partir de uma amostra está sujeito a aumentar a variância dessa estimativa, isto é, a modelar ruído que não generaliza para outras amostras.\nNesta aula, estudaremos um método data-driven para encontrar o ponto ótimo deste trade-off: tuning. Com ele, poderemos identificar melhores combinações de modelos, hiper-paramêtros e features para reduzir viés e variância simultaneamente. Por hiper-parâmetros geralmente nos referimos a configurações que podemos fazer nos modelos para customizar a forma com que parâmetros (i.e., coeficientes ou pesos) serão estimados. Por tuning, por sua vez, indicamos o processo de otimizar uma função complexa (ou black-box) que possui diferentes inputs – caso de uma pipeline – para encontrar o melhor conjunto de hiper-paramêtros e etapas de pré-processamento com a finalidade de minimizar erro em uma estratégia de validação. Tuning, por essa razão, depende de uma boa estratégia de validação (ver a discussão de Neunhoeffer e Sternberg (2019) sobre esse ponto)."
  },
  {
    "objectID": "materiais/aula10.html#gridsearch",
    "href": "materiais/aula10.html#gridsearch",
    "title": "Aula 10",
    "section": "Gridsearch",
    "text": "Gridsearch\nO algoritmo mais básico para encontrarmos configurações de pipelines úteis é o grid search, que nada mais é o do que o teste exaustivo de todas as combinações possíveis de hiper-parâmetros pré-especificadas. Um exemplo: imagine que queremos testar diferentes versões de um Random Forest (visto na aula 7), com proporção maior ou menor de features a reter1 e com maior ou menor número de árvores a serem treinadas. Com grid search, é fácil implementar este teste tanto em R quanto em Python:1 Como vimos, RF usa um processo de sorteio de variáveis a usar para aumentar a diversidade das árvores fracas, os weak learners.\n\nRPython\n\n\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(tidyverse)\n\nlink <- \"https://raw.githubusercontent.com/FLS-6497/datasets/main/aula7/eleicoes2000.csv\"\ndados <- readr::read_csv2(link) %>%\n  select(-cod_mun_ibge, -nome_municipio) %>%\n  mutate_if(is.character, as.factor)\n\n# Define a task\ntsk <- as_task_classif(partido ~ ., data = dados, positive = \"PMDB-PSDB-PFL\")\n\n# Cria uma pipeline (e indica parametros para tuning)\ngr <- po(\"learner\", learner = lrn(\"classif.randomForest\"),\n         ntree = to_tune(c(20, 50, 100)),\n         mtry = to_tune(c(3, 7, 11))) %>%\n  as_learner()\n\n# Criamos uma instancia (parecido com um design grid)\ninstance <- ti(\n  task = tsk,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.fbeta\"),\n  terminator = trm(\"none\")\n)\n\n# Tuning\ntuner <- tnr(\"grid_search\")\ntuner$optimize(instance)\n\n# Os resultados ficam salvos em um tibble\nas.data.table(instance$archive) %>%\n  as_tibble()\n\n# Retreina a melhor pipeline na base completa\ngr$param_set$values <- instance$result_learner_param_vals\ngr$train(tsk)\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\n\n# Carrega os dados\nlink = 'https://raw.githubusercontent.com/FLS-6497/datasets/main/aula7/eleicoes2000.csv'\ndados = pd.read_csv(link, sep=';', decimal=\",\").drop(['cod_mun_ibge', 'nome_municipio'], axis=1)\ndados['partido'] = np.where(dados['partido']=='Outros', 0, 1)\n\n# Separa target e features\nY = dados['partido']\nX = dados.loc[:, dados.columns != 'partido']\n\n# Cria uma pipeline\npipe = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore')), ('rf', RandomForestClassifier())])\n\n# Grid de parametros para testar\nparam_grid = [\n{'rf__n_estimators': [20, 50, 100], 'rf__max_features': [3, 7, 11]}\n]\n\n# Roda o gridsearch usando 5-fold CV\nres = GridSearchCV(pipe, param_grid, cv=5, scoring='f1')\nres.fit(X, Y)\n\n# Acessa a melhor pipeline\nres.best_estimator_\nres.best_score_\n\n\n\nHá dois detalhes importantes a notar quanto ao procedimento acima. Em primeiro lugar, é necessário especificar quais hiper-parâmetros iremos testar; em R, fazemos isso com a função to_tune e, em Python, usando um dicionário. Isso declarado, a função ou classe de grid search testará todas as combinações possíveis de hiper-parâmetros e retornará scores, de acordo com a métrica de validação definida, para identificarmos qual foi a melhor combinação entre as testadas.2 Em segundo lugar, grid search calcula scores com base em alguma estratégia de validação, geralmente K-fold cross validation com \\(k=5\\) por padrão – mas vale notar que essa nem sempre é a melhor estratégia, como vimos na aula anterior.2 Note que, em R, o mlr3 retorna as melhores configurações e, ao fim, é necessário retreinar o modelo nos dados completos; em Python, o sklearn já treina as melhores configurações em toda a base ao final se o argumento refit=True for declarado (o que é o default)."
  },
  {
    "objectID": "materiais/aula10.html#random-gridsearch",
    "href": "materiais/aula10.html#random-gridsearch",
    "title": "Aula 10",
    "section": "Random gridsearch",
    "text": "Random gridsearch\nQuando temos muitas combinações possíveis de hiper-parâmetros para testar, explorar cada uma delas pode ser inviável. Por conta disso, outra estratégia comum de tuning é sortear aleatoriamente apenas algumas combinações para teste. Em particular, geralmente esse procedimento é feito definindo-se um espaço de hiper-parâmetros maior. Implementar essa estratégia pode ser feito assim:\n\nRPython\n\n\n# Cria uma pipeline com um espaço de hiper-parametros maior\ngr <- po(\"learner\", learner = lrn(\"classif.randomForest\"),\n         ntree = to_tune(lower = 10, upper = 300),\n         mtry = to_tune(lower = 3, upper = 11)) %>%\n  as_learner()\n\n# Criamos uma instancia\ninstance <- ti(\n  task = tsk,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.fbeta\"),\n  terminator = trm(\"evals\", n_evals = 10)\n)\n\n# Tuning\ntuner <- tnr(\"random_search\")\ntuner$optimize(instance)\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Grid de parametros para testar\nparam_grid = [\n{'rf__n_estimators': list(range(10, 300)), 'rf__max_features': list(range(3, 11))}\n]\n\nres = RandomizedSearchCV(pipe, param_grid, cv=5, scoring='f1', n_iter=3)\nres.fit(X, Y)"
  },
  {
    "objectID": "materiais/aula10.html#otimização-bayesiana",
    "href": "materiais/aula10.html#otimização-bayesiana",
    "title": "Aula 10",
    "section": "Otimização bayesiana",
    "text": "Otimização bayesiana\nGrid search e random grid search, como dá para perceber pelos exemplos anteriores, não são as formas mais eficientes de se encontrar bons hiper-parâmetros – ambas gastam muito tempo e recursos investigando configurações ruins, isto é, elas não adotam otimização para fazer uma busca eficiente por uma configuração ideal. Há diferentes soluções que atacam este problema mas, tanto por ser mais utilizada quanto por ter implementação fácil em R e Python, uma que estudaremos em maior detalhe agora é a otimização bayesiana.33 Para quem usa Python, será necessário instalar o pacote scikit-optimize com pip install scikit-optimize no terminal. Para quem usa R, instalaremos o pacote mlr3mbo e o DiceKriging com install.packages(c(\"mlr3mbo\", \"DiceKriging\")).\nDe forma resumida, otimização bayesiana é uma maneira de encontrar, rápida e eficientemente, boas configurações dentro de um espaço de hiper-parâmetros potencialmente grande. Para tanto, a otimização assume um prior vago sobre a função sendo otimizada e, depois de ver dados de validação das primeiras interações, atualiza a posterior para sugerir próximos valores mais promissores a serem testados – o que é diferente de selecionar aleatória ou sequencialmente combinações a serem testadas, como ocorre com random grid search ou grid search, respectivamente. É por isso que, no geral, otimização bayesiana tende a reduzir o tempo de tuning, especialmente quando há um espaço de hiper-parâmetros muito grande e quando a pipeline que usamos é complexa (e lenta para treinar). Implementá-la em R ou em Python é bastante simples utilizando os nossos frameworks:\n\nRPython\n\n\n# Criamos uma instancia\ninstance <- ti(\n  task = tsk,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.fbeta\"),\n  terminator = trm(\"evals\", n_evals = 10)\n)\n\n# Tuning\ntuner <- tnr(\"mbo\")\ntuner$optimize(instance)\n\n\nfrom skopt import BayesSearchCV\n\n# Grid de parametros para testar\nparam_grid = [\n{'rf__n_estimators': list(range(10, 300)), 'rf__max_features': list(range(3, 11))}\n]\n\nres = BayesSearchCV(pipe, param_grid, cv=5, scoring='f1', n_iter = 10)\nres.fit(X, Y)"
  },
  {
    "objectID": "materiais/aula10.html#stoppping",
    "href": "materiais/aula10.html#stoppping",
    "title": "Aula 10",
    "section": "Stoppping",
    "text": "Stoppping\nSe tivermos um espaço de hiper-parâmetros muito grande, ou se quisermos evitar testar mais combinações do que o necessário ou do que o possível em um dado período de tempo, pode ser útil ter formas de controlar o processo de tuning. Comumente, isso é feito por meio da definição de algum critério, como o de número máximo de combinações de hiper-parâmetros a testar; ou o de definir um limite de tempo de tuning a partir do qual ele será encerrado; ou, ainda, o de encerrar quando progressos não são mais alcançados. Neste aspecto, o mlr3 oferece muitas mais opções do que o sklearn:\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nIterações\ntrm(\"evals\", n_evals = 10)\nn_iter=10\n\n\nTempo\ntrm(\"run_time\", secs = 100)\n\n\n\nPerformance\ntrm(\"perf_reached\", level = 0)\n\n\n\nEstagnação\ntrm(\"stagnation\", iters = 10, threshold = 0.01)\n\n\n\n\nAlguns exemplos usando o mlr3:\n# Parar depois de 10 minutos\ninstance1 <- ti(\n  task = tsk,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.fbeta\"),\n  terminator = trm(\"run_time\", secs = 600)\n)\n\n# Parar se atingir F1 de 0.8, senao continua ate esgotar o grid\ninstance2 <- ti(\n  task = tsk,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.fbeta\"),\n  terminator = trm(\"perf_reached\", level = 0.7)\n)\n\n# Para se nao melhorar mais que 0.01 apos 10 iteracoes\ninstance3 <- ti(\n  task = tsk,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.fbeta\"),\n  terminator = trm(\"stagnation\", iters = 10, threshold = 0.01)\n)"
  },
  {
    "objectID": "materiais/aula10.html#espaço-de-hiper-parâmetros",
    "href": "materiais/aula10.html#espaço-de-hiper-parâmetros",
    "title": "Aula 10",
    "section": "Espaço de hiper-parâmetros",
    "text": "Espaço de hiper-parâmetros\nComo selecionar valores de hiper-parâmetros para tuning? Esta não é uma pergunta simples de ser respondida porque, a depender do modelo utilizado e de etapas de pré-processamento, diferentes hiper-parâmetros estão disponíveis. Além de materiais na internet que podem ser úteis, o mlr3 tem uma biblioteca auxiliar que contém uma série de dicionários já pré-programados, com valores apropriados para diferentes tipos de tarefas, que podem ser implementados facilmente em um projeto por meio da função lts:\ngr <- lts(lrn(\"classif.rpart\"))\n\ninstance <- ti(\n  task = tsk,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.fbeta\"),\n  terminator = trm(\"evals\", n_evals = 25)\n)\n\n# Tuning\ntuner <- tnr(\"random_search\")\ntuner$optimize(instance)\n\nas.data.table(instance$archive) %>%\n  as_tibble()"
  },
  {
    "objectID": "materiais/aula10.html#outras-estratégias",
    "href": "materiais/aula10.html#outras-estratégias",
    "title": "Aula 10",
    "section": "Outras estratégias",
    "text": "Outras estratégias\nGrid search e random grid search são duas estratégias muito comuns e simples de tuning. Otimização bayesiana, como vimos anteriormente, vai um pouco além ao considerar de forma mais eficiente boas configurações para investigar sequencialmente. Além destas estratégias, há várias outras, algumas recentes, que podem ser tão ou mais úteis mas que, pelo tempo reduzido, não exploraremos.\nDestas estratégias, o sklearn implementa o halvening, que vai descartando combinações que se saem pior em parcelas menores dos dados; mas também é possível utilizar pacotes externos para implementar outras. O mlr3, por sua vez, já contém várias alternativas. Uma lista pode ser consultada aqui. Um exemplo de implementação de tuning utilizando irace:\ngr <- lts(lrn(\"classif.rpart\"))\n\ninstance <- ti(\n  task = tsk,\n  learner = gr,\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.fbeta\"),\n  terminator = trm(\"evals\", n_evals = 100)\n)\n\n# Tuning\ntuner <- tnr(\"irace\")\ntuner$optimize(instance)"
  },
  {
    "objectID": "materiais/aula10.html#paralelização",
    "href": "materiais/aula10.html#paralelização",
    "title": "Aula 10",
    "section": "Paralelização",
    "text": "Paralelização\nTuning é algo que costuma tomar bastante tempo de processamento. No caso em que utilizamos computadores com mais de um CPU – praticamente qualquer computador hoje em dia – podemos nos valer disso para implementar paralelização, isto é, para dividir o processamento do teste de diferentes combinações de hiper-parâmetros. A depender da capacidade de processamento do seu computador, paralelização pode facilmente reduzir em várias vezes o tempo levado para tuning de um pipeline.\nCom o mlr3, é necessário usar o pacote future para usar paralelização.4 Com ele instalado, o processo é simples: basta incluir uma chamada à função plan (workers indica o número de threads a serem usadas):4 Paralelização em R, de forma geral, é algo mais complicado e no mais das vezes requer soluções específicas. Para uma discussão mais detida sobre como o mlr3 implementa paralelização, ver o capítulo 8 do mlr3 book.\nfuture::plan(\"multisession\", workers = 4)\nEm Python, todas as classes de tuning que vimos contêm um argumento chamado n_jobs para controlar o número de CPUs a serem utilizados (é possível definir n_jobs=-1 para usar todos os núcleos disponíveis)."
  },
  {
    "objectID": "materiais/aula10.html#reprodutibilidade",
    "href": "materiais/aula10.html#reprodutibilidade",
    "title": "Aula 10",
    "section": "Reprodutibilidade",
    "text": "Reprodutibilidade\nTuning e resampling são processos que envolvem processos aleatórios de sorteios.5 Por conta disso, rodar um workflow inteiro duas vezes pode produzir resultados distintos. Para evitar isso, uma boa prática é utilizar seeds, isto é, um número de inicialização que garante a reprodução de um sorteio ou de uma sequência de sorteios.5 Alguns algoritmos também utilizam sorteio para iniciar parâmetros, caso de algoritmos de boosting e bagging, por exemplo.\nEm R, é possível definir um seed com set.seed(123), por exemplo, logo no início de um script. Em Python, é possível usar numpy para definir um seed global com np.random.seed(123), mas várias classes também possuem um argumento random_state que permitem definir seeds locais, o que também é algo recomendado a se fazer."
  },
  {
    "objectID": "materiais/aula11.html",
    "href": "materiais/aula11.html",
    "title": "Aula 11",
    "section": "",
    "text": "Na aula de hoje, veremos o básico de um tipo diferente de modelo de aprendizado de máquina que alterou profundamente áreas como em visão computacional e processamento de linguagem natural: modelos de aprendizado profundo. Mais conhecidos como modelos de deep learning, seu aspecto característico é ser formado por diferentes camadas (layers) de células (ou neurons) que recebem diferentes inputs e os transformam aplicando funções não-lineares – daí o deep no nome, dado que essas camadas geralmente são empilhadas e acabam formando arquiteturas como a abaixo:\n\n\n\nRede neural com uma camada oculta\n\n\nDeep learning é um tema vasto e inclui diferentes tipos de arquitetura de redes para atacar diferentes tipos de problema. O que cobriremos para introduzir o tópico, no entanto, é uma aplicação partilar cuja performance ajudou a popularizar o aprendizado profundo: a classificação de imagens. Para isso, veremos redes neurais convolucionais, que têm arquiteturas adaptadas para processar dados de que tem uma estrutura de vizinhança na qual ocorrências adjacentes importam para predizermos uma determinada característica – como a presença de padrões horizontais ou verticais de determinada cor, ou ainda círculos e curvas."
  },
  {
    "objectID": "materiais/aula11.html#como-redes-são-treinadas",
    "href": "materiais/aula11.html#como-redes-são-treinadas",
    "title": "Aula 11",
    "section": "Como redes são treinadas",
    "text": "Como redes são treinadas\nHá diferenças importantes na forma com que treinamos uma rede neural em relação ao que vimos anteriormente neste curso. Em primeiro lugar, no mais das vezes redes neurais usam lotes (ou batches) de uma amostra para treino, isto é, apenas parcelas menores das amostras de treino são passadas para a rede por vez, o que tende a ser mais rápido. Em vez do treino ser feito de uma só vez, portanto, a amostra de treino é passada ao modelo em pequenas parcelas – a volta completa da rede pela amostra de treino é chamada de época (ou epoch). Por conta do número grande parâmetros a serem estimados (cada um dos \\(w\\) e \\(\\beta\\) que vimos antes), redes neurais comumente são treinadas usando-se mais de uma época, mas, quando mais épocas são utilizadas, maior o risco de overfitting.\nEm segundo lugar, redes neurais podem usar diferentes algoritmos de otimização para encontrar os melhores parâmetros para uma dada tarefa. A principal especificidade aqui é que o processo de ajuste de parâmetros mais utilizado em aplicações é feito de trás para frente (algo chamado de backpropagation). Após cada batch, a rede faz predições e calcula alguma métrica de validação para avaliar seu desempenho e, do último à primeira camada, vai fazendo ajustes nos parâmetros de forma a corrigir desvios.\nFinalmente, por terem uma estrutura tão mais complexa do que a de modelos tradicionais de aprendizado de máquina, redes neurais acabam dependendo de uma quantidade maior de dados para serem treinados de forma eficiente.11 Há exceções aqui, além de práticas como a de ajustar modelos base já treinados em bases grandes de dados para resolver tarefas mais específicas, estas passadas a partir de amostras menores. Em aprendizado profundo, essa prática de reutilizar um modelo para uma tarefa diferente daquela em que ele foi originalmente treinado é chamado de transfer learning."
  },
  {
    "objectID": "materiais/aula11.html#instalação",
    "href": "materiais/aula11.html#instalação",
    "title": "Aula 11",
    "section": "Instalação",
    "text": "Instalação\nAntes de começarmos a criar e a treinar redes neurais com Keras, precisaremos instalar o framework (a essa altura, espero que todos e todas já tenham feito isso em seus computadores pessoais, mas teremos que fazer o mesmo nos computadores em sala de aula).3 O processo é bastente simples, embora possa retornar diferentes tipos de erro:3 No R, é também possível instalar o Tensorflow primeiro, biblioteca da qual o Keras depende, mas instalar o Keras diretamente produz o mesmo resultado.\n\nRPython\n\n\n# Instala o pacote Keras\ninstall.packages(\"keras\")\n\n# Instala o Keras\nkeras::install_keras()\n\n# Para quem usa OSX, talvez uma das opções abaixo seja necessário\nkeras::install_keras(method = \"virtualenv\")\nkeras::install_keras(method = \"conda\")\n\n\n# O ideal é seguir aqui: https://www.tensorflow.org/install/pip\npython3 -m pip install tensorflow"
  },
  {
    "objectID": "materiais/aula11.html#uma-rede-simples",
    "href": "materiais/aula11.html#uma-rede-simples",
    "title": "Aula 11",
    "section": "Uma rede simples",
    "text": "Uma rede simples\nNesta aula, trabalharemos com um dos bancos de dados mais populares na área: o MNIST, que é uma base anotada com 60000 mil imagens de dígitos numéricos escritos a mão. Nosso objetivo, portanto, será o de predizer, a partir de imagens, os dígitos numéricos de representam. Exemplos de imagens do MNIST:\nPara implementar uma rede neural com uma única camada oculta e 32 unidades4, podemos usar o seguinte código:4 Na verdade, temos uma segunda para comprimir os resultados da primeira.\n\nRPython\n\n\n\nlibrary(keras)\n\n# Carrega as imagens do MNIST\nmnist <- dataset_mnist()\n\n# Separamos 5 mil imagens para validacao\nX_treino <- mnist$train$x[1:55000,,]\nX_validacao <- mnist$train$x[55001:60000,,]\nX_teste <- mnist$test$x\n\ny_treino <- mnist$train$y[1:55000]\ny_validacao <- mnist$train$y[55001:60000]\ny_teste <- mnist$test$y\n\n# Aqui, precisamos alterar a estrutura da matriz que armazena os dados \n# (28 x 28 pixels cada uma possui)\nX_treino <- array_reshape(X_treino, c(nrow(X_treino), 28, 28, 1))\nX_validacao <- array_reshape(X_validacao, c(nrow(X_validacao), 28, 28, 1))\nX_teste <- array_reshape(X_teste, c(nrow(X_teste), 28, 28, 1))\n\n# As cores são representadas de 1 a 255\n# Com esse passo, deixamos todas variarem de 0 a 1 (como scale)\nX_treino <- X_treino / 255\nX_validacao <- X_validacao / 255\nX_teste <- X_teste / 255\n\n# Converte os targets para one-hot\ny_treino <- to_categorical(y_treino, num_classes = 10)\ny_validacao <- to_categorical(y_validacao, num_classes = 10)\ny_teste <- to_categorical(y_teste, num_classes = 10)\n\n# Define a arquitetura do modelo\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_dense(units = 32, activation = \"relu\", input_shape = c(28, 28, 1)) %>%\n  layer_flatten() %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\n# Compila o modelo\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n# Treina o modelo\nmodel %>% fit(\n  x = X_treino, y = y_treino,\n  validation_data = list(X_validacao, y_validacao),\n  epochs = 10\n)\n\n# Faz predicoes na amostra de teste\nmodel %>% \n  evaluate(X_teste, y_teste)\n\n\nimport numpy as np\nfrom tensorflow import keras\n\n# Carrega as imagens do MNIST\n(X_treino, y_treino), (X_teste, y_teste) = keras.datasets.mnist.load_data()\n\n# Separamos 5 mil imagens para validacao\nX_treino, X_validacao = X_treino[:55000], X_treino[55000:]\ny_treino, y_validacao = y_treino[:55000], y_treino[55000:]\n\n# Aqui, precisamos alterar a estrutura da matriz que armazena os dados \n# (28 x 28 pixels cada uma possui)\nX_treino = X_treino.reshape(55000, 28, 28, 1)\nX_validacao = X_validacao.reshape(5000, 28, 28, 1)\nX_teste = X_teste.reshape(10000, 28, 28, 1)\n\n# As cores são representadas de 1 a 255\n# Com esse passo, deixamos todas variarem de 0 a 1 (como scale)\nX_treino = X_treino / 255\nX_validacao = X_validacao / 255\nX_teste = X_teste / 255\n\n# Converte os targets para one-hot\ny_treino = keras.utils.to_categorical(y_treino, 10)\ny_validacao = keras.utils.to_categorical(y_validacao, 10)\ny_teste = keras.utils.to_categorical(y_teste, 10)\n\n# Define a arquitetura do modelo\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Dense(32, activation=\"relu\", input_shape=(28, 28, 1)))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\n# Compila o modelo\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# Treina o modelo\nmodel.fit(X_treino, y_treino, batch_size=64, epochs=10, validation_data=(X_validacao, y_validacao))\n\n# Faz predicoes na amostra de teste\nmodel.evaluate(X_teste, y_teste)\n\n\n\nO código é extenso e contém muitas coisas importantes. A primeira parte importante é o pré-processamento das imagens, que discutiremos mais adiante – precisamos transformá-las em um formato apropriado. Mais importante, no entanto, precisamos definir a arquitetura do modelo e, para isso, usamos a função/classe sequential que é a base para empilharmos diferentes camadas para formar uma rede. No caso específico, definimos uma camada de inputs que espera recebê-los com as dimensões \\((28, 28, 1)\\) (imagens em cinza de 28 por 28 pixels); depois definimos uma camada oculta com 32 unidades, com a função de ativação ReLU (com ela, valores negativos serão censurados em 0); depois, dado que os dados têm um formato de duas dimensões, usamos uma camada de compressão para transformar os outputs anteriores em vetores, estes passados para a camada de outputs que retorna as probabilidades de cada imagem serem de um dado dígito.\nNovamente, a rede que definimos anteriormente é bem simples e pode ser melhorada com mudanças simples, como aumentar o número de unidades (de 32 para 128 ou 256, por exemplo), ou ainda adicionar mais camadas ocultas, o que permite à rede aprender padrões mais complexos. De toda forma, há algo melhor a se fazer, que é o que veremos em seguida."
  },
  {
    "objectID": "materiais/aula11.html#redes-convolucionais",
    "href": "materiais/aula11.html#redes-convolucionais",
    "title": "Aula 11",
    "section": "Redes convolucionais",
    "text": "Redes convolucionais\nUm tipo de arquitetura bastante utilizada para problemas que envolvem imagens é de redes neurais convolucionais (CNN, na sigla em inglês). De específico, essas redes contêm camadas que aplicam transformações de convolução nos inputs que recebem. O processo é mais ou menos este: cada camada possui um número de filtros, que nada mais são do que componentes pequenos que se especializam em detectar determinado padrão, isto é, a isolar partes menores das imagens que recebem como inputs5; para tanto, o processo de convolução é utilizado para fazer com que os filtros se movimentem em diferentes partes da imagem até encontrar padrões que ajudem a minimizar os erros do modelo. Dessa forma, cada filtro, iniciado com pesos aleatórios, se especializará em determinar determinado tipo específico de padrão que, posteriormente, poderá ser agregado por camadas com número menor de unidades.5 De forma mais técnica, cada camada convolucional precisa especificar o número de filtros que possuirá e as dimensões desses filtros (por exemplo, se será uma matrix \\(5x\\)).\nUm exemplo mais intituivo de como filtros em camadas de convolução funcionam pode ser visto abaixo. Na primeira parte da matriz da esquerda (que representa uma imagem), é possível ver que todas as células contêm o valor 10, o que indica uma única cor sólida em toda a área; a matriz de pesos, \\(3 x 3\\), multiplica cada um desses valores e os agrega, retornando 0; em outra parte da imagem, indicada por vermelho, entretanto, o valor retornado da multiplicação das matrizes é 30, o que indica que há um padrão vertical presente. Em outras palavras, os pesos do filtro \\(3x3\\) do exemplo se especializaram em detectar padrões verticais e, sempre que os encontram, retornam um valor substantivo (que é retornado por uma matriz menor, \\(4 x 4\\)).\n\n\n\nExemplo de filtros em CNNs\n\n\nImplementar CNNs, portanto, passa por especificar uma camada convolucional com um determinado número de filtros de dimensões específicas. Usando Keras, um exemplo de aplicação de uma rede com essa arquitetura para predizer as imagens do MNIST segue abaixo:\n\nRPython\n\n\n# Define uma rede convolucional (filtro de 3x3)\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3),\n                activation = \"relu\", input_shape = c(28, 28, 1)) %>%\n  layer_flatten() %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\n# Compila o modelo\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n# Treina\nmodel %>% fit(\n  x = X_treino, y = y_treino,\n  validation_data = list(X_validacao, y_validacao),\n  epochs = 10\n)\n\n# Avalia\nmodel %>% \n  evaluate(X_teste, y_teste)\n\n\n# Define a arquitetura do modelo\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', input_shape=(28,28,1)))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n\n# Compila o modelo\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# Treina\nmodel.fit(X_treino, y_treino, batch_size=64, epochs=10, validation_data=(X_validacao, y_validacao))\n\n# Avalia\nmodel.evaluate(X_teste, y_teste)\n\n\n\nO resultado dessa simples alteração de hidden layer é um aumento enorme em acurácia (que vocês poderão testar), e isso ainda não considera melhor possíveis como usar um maior número de unidades ocultas, aumentar o tamanho dos filtros ou adicionar mais camadas."
  },
  {
    "objectID": "materiais/projeto2.html",
    "href": "materiais/projeto2.html",
    "title": "Projeto 2",
    "section": "",
    "text": "Para este projeto, você pode usar a arquitetura de modelo que quiser, com hiper-parâmetros que julgar adequados. O único requisito é usar Keras e não utilizar modelos pré-treinados (transfer learning).\n\nDados\nUsaremos um conjunto de 5000 imagens de satélite de locais de votação no Brasil que estão no GitHub do curso. Baixe eles no seu computador (a maneira mais fácil é clonar o repositório inteiro de datasets, ou então clicar em Download ZIP no botão verde no canto superior direito do repositório).\nA pasta está organizada da seguinte forma:\nmapas\n└───treino\n│   └───rural\n│   └───urbano\n└───teste\n│   └───rural\n│   └───urbano\n└───validacao\n│   └───rural\n│   └───urbano\n\nDentro da pasta treino há 4000 imagens e, dentro de teste e validacao, 500 em cada. Cada uma dessas pastas está dividida em rural e urbano, indicando o label das imagens. Para carregá-las, basta usar a função image_dataset_from_directory que vimos em aula.\n\n\nObjetivo\nO objetivo central dessa atividade é treinar um modelo de deep learning que consiga classificar imagens de locais de votação como sendo URBANAS ou RURAIS.\n\n\nEntrega\nA entrega deverá ser feita na pasta do GitHub de cada um contendo:\n\nUm notebook ou script com o código utilizado;\nUm documento (pode ser um PDF compilado pelo notebook, mas também pode ser um README.md) detalhando a metodologia utilizada:\n\nArquitetura utilizada\nHiper-parâmetros utilizados\nResumo dos resultados obtidos\nPredição para a base de validação com imagens de satélite sem labels"
  },
  {
    "objectID": "temp.html",
    "href": "temp.html",
    "title": "Materiais",
    "section": "",
    "text": "flowchart LR\n  A[Pergunta] --> B[Problema]\n  B --> C{Decision}\n  C --> D[Result one]\n  C --> E[Result two]"
  },
  {
    "objectID": "temp.html#básico",
    "href": "temp.html#básico",
    "title": "Materiais",
    "section": "Básico",
    "text": "Básico\n\nRPython\n\n\nfizz_buzz <- function(fbnums = 1:50) {\n  output <- dplyr::case_when(\n    fbnums %% 15 == 0 ~ \"FizzBuzz\",\n    fbnums %% 3 == 0 ~ \"Fizz\",\n    fbnums %% 5 == 0 ~ \"Buzz\",\n    TRUE ~ as.character(fbnums)\n  )\n  print(output)\n}\n\n\ndef fizz_buzz(num):\n  if num % 15 == 0:\n    print(\"FizzBuzz\")\n  elif num % 5 == 0:\n    print(\"Buzz\")\n  elif num % 3 == 0:\n    print(\"Fizz\")\n  else:\n    print(num)"
  },
  {
    "objectID": "recursos.html",
    "href": "recursos.html",
    "title": "Recursos",
    "section": "",
    "text": "Para acompanhar o curso, conhecimentos intermediários em programaçãosão necessários. Nesta página, listo algumas referências e recursos úteis para o aprimoramento desses pré-requisitos."
  },
  {
    "objectID": "recursos.html#programação",
    "href": "recursos.html#programação",
    "title": "Recursos",
    "section": "Programação",
    "text": "Programação\n\nR\n\nR for Data Science, Garrett Grolemund and Hadley Wickham.\n\nLivro essencial, cobre o básico até o intermediário do uso de R e do tidyverse aplicados à Ciência de Dados\n\nR Cookbook, Winston Chang\n\nLivro prático focado na resolução de problemas comuns\n\nRStudio cheatsheets\n\nGuias práticos para resolução de diferentes problemas com tidyverse\n\nRweekly\n\nNewsletter semanal com novidades sobre R\n\n\n\n\nPython\n\nPython for Data Analysis, Wes McKinney\n\nEm certo sentido, é um livro similar ao R4DS, bom para iniciantes\n\nLearn Python\n\nSite com diversos tutoriais e textos sobre Python\n\nPython cheatsheets\n\nGuias práticos, agora para Python\n\nPycoders\n\nUma das mais lidas newsletters sobre Python"
  },
  {
    "objectID": "exercicios/exercicios2.html",
    "href": "exercicios/exercicios2.html",
    "title": "Exercícios 2",
    "section": "",
    "text": "Para esse exercício, será necessário carregar alguns dados climáticos de São Bernardo do Campo (SP):\n\nRPython\n\n\nlink <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv\"\ndados <- readr::read_csv(link)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv'\ndados = pd.read_csv(link)\n\n\n\n\n\nAnalise a relação entre entre cobertura de nuvens (cloud_coverage) e temperatura máxima (maximum_temperature). Para isso, plote gráficos com a relação bivariada, use coeficiente de correlação ou um modelo linear (OLS). Descreva os resultados que encontrar.\n\n\n\nExiste alguma outra variável na base com maior correção com a temperatura máxima? Novamente, registre os resultados que encontrar.\n\n\n\nCrie um código que faça um gráfico da relação bivariada entre todas as variáveis contínuas na base e os salve em disco. Dica:\n\nRPython\n\n\nlibrary(tidyverse)\n\np <- ggplot()\nggsave(p, file = paste0(\"grafico.png\"))\n\n\nfrom matplotlib import pyplot as plt\nplt.savefig('grafico.png')\n\n\n\n\n\n\nRode modelos lineares simples (por mínimos quadrados ordinários) para predizer a temperatura máxima diária em São Bernardo do Campo (SP). Use as variáveis que quiser, faça transformações nelas se necessário, e reporte alguns resultados do melhor modelo que encontrar.\n\n\n\nSalve as predições do seu modelo treinado no exercício anterior e compare com os valores reais de temperatura máxima (vale usar gráficos)."
  },
  {
    "objectID": "exercicios/exercicios2.html#a-umidade",
    "href": "exercicios/exercicios2.html#a-umidade",
    "title": "Exercícios 2",
    "section": "a) Umidade",
    "text": "a) Umidade\nCrie uma função (ou um código) para sortear 1000 observações do banco de dados climáticos, calcular a média de umidade (humidity)."
  },
  {
    "objectID": "exercicios/exercicios2.html#b-histograma",
    "href": "exercicios/exercicios2.html#b-histograma",
    "title": "Exercícios 2",
    "section": "b) Histograma",
    "text": "b) Histograma\nCom a função criada anteriormente, calcule 1000 médias de amostras de humidity e plote a distribuição como um histograma."
  },
  {
    "objectID": "exercicios/exercicios2.html#c-modelos-lineares",
    "href": "exercicios/exercicios2.html#c-modelos-lineares",
    "title": "Exercícios 2",
    "section": "c) Modelos lineares",
    "text": "c) Modelos lineares\nModifique a função criada anteriormente para, depois de sortear 1000 observações do banco, rodar um modelo de regressão linear para predizer valores de humidity e extrair o r2 do modelo. Dica:\n\nRPython\n\n\nmodelo <- lm(rnorm(100) ~ rnorm(100))\nsummary(modelo)$r.squared\n\n\nfrom matplotlib import pyplot as plt\nplt.savefig('grafico.png')"
  },
  {
    "objectID": "exercicios/exercicios3.html",
    "href": "exercicios/exercicios3.html",
    "title": "Exercícios 3",
    "section": "",
    "text": "Para esse exercício, será necessário carregar alguns dados sobre violência policial letal nos Estados Unidos:\n\nRPython\n\n\nlink <- \"https://raw.githubusercontent.com/FLS-6497/datasets/main/aula3/PKAP_raw_data.csv\"\ndados <- readr::read_csv(link)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/FLS-6497/datasets/main/aula3/PKAP_raw_data.csv'\nc = pd.read_csv(link)\n\n\n\nPara quem usa R, também será importante instalar algumas bibliotecas previamente:\ninstall.packages(c(\"mlr3verse\", \"mlr3measures\"))\n\n\nFaça gráficos de barras com a frequência da variável race por cada uma das variáveis officer_ na base de dados. O resultado deve indicar quantas vítimas de mortes por violência letal policial de diferentes raças (whites e blacks) ocorreram em diferentes categorias (e.g., office_offduty). Dica: algumas variáveis precisam ser recategorizadas porque possuem muitas categorias com poucas ocorrências.\n\n\n\nCrie uma nova base de dados que inclua apenas as variáveis mencionadas na Tabela 1 do paper de Streeter. Dica: será necessário criar novas variáveis e descartar outras existentes na base. Para quem usa Python, também é importante recodificar variáveis para o formato de one hot encoding (em R, a maioria das funções de regressão já faz essa conversão por baixo dos panos)."
  },
  {
    "objectID": "exercicios/exercicios3.html#a-criar-função",
    "href": "exercicios/exercicios3.html#a-criar-função",
    "title": "Exercícios 3",
    "section": "a) Criar função",
    "text": "a) Criar função\nCrie uma função para sortear da base uma amostra de treino e, outra, de teste. Para isso, a função pode retornar uma lista com as duas amostras. Crie também um argumento na função que permita selecionar o percentual de observações na amostra de treino (o default precisará ser 0.7)."
  },
  {
    "objectID": "exercicios/exercicios3.html#b-modelo-com-treino-e-teste",
    "href": "exercicios/exercicios3.html#b-modelo-com-treino-e-teste",
    "title": "Exercícios 3",
    "section": "b) Modelo com treino e teste",
    "text": "b) Modelo com treino e teste\nCom a função anterior, retreine seu modelo anterior na amostra de treino e, depois, aplique as predições na amostra de teste."
  },
  {
    "objectID": "exercicios/exercicios3.html#c-tamanho-das-amostras-de-treino",
    "href": "exercicios/exercicios3.html#c-tamanho-das-amostras-de-treino",
    "title": "Exercícios 3",
    "section": "c) Tamanho das amostras de treino",
    "text": "c) Tamanho das amostras de treino\nCom a função anterior, retreine seu modelo usando diferentes tamanhos de amostra de treino, de 0.3 a 0.9 com intervalos de 0.05. Crie um gráfico para reportar alguma métrica de validação (pode ser acurácia ou precisão, ou ainda F1) e, no eixo X, inclua a informação sobre o percentual usado"
  },
  {
    "objectID": "exercicios/exercicios3.html#a-nova-função",
    "href": "exercicios/exercicios3.html#a-nova-função",
    "title": "Exercícios 3",
    "section": "a) Nova função",
    "text": "a) Nova função\nModifique a função criada anteriormente para que ela já separe a amostra em treino e teste, rode um modelo logístico e retorne alguma métrica de validação."
  },
  {
    "objectID": "exercicios/exercicios3.html#b-cross-validation",
    "href": "exercicios/exercicios3.html#b-cross-validation",
    "title": "Exercícios 3",
    "section": "b) Cross-validation",
    "text": "b) Cross-validation\nUse a função criada anteriormente para rodar 500 modelos logísticos em diferentes amostras de treino e de teste. Reporte os resultados desse exercício com um histograma dos valores de validação de alguma métrica."
  },
  {
    "objectID": "exercicios/exercicios4.html",
    "href": "exercicios/exercicios4.html",
    "title": "Exercícios 4",
    "section": "",
    "text": "Para esse exercício, será necessário carregar alguns dados que já vimos sobre o clima em São Bernardo do Campo (SP):\n\nRPython\n\n\nlink <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv\"\ndados <- readr::read_csv(link)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv'\nc = pd.read_csv(link)\n\n\n\nTambém usaremos, de ponta a ponta, os frameworks que estamos estudando. Para rodar um modelo de regressão OLS com partição da amostra entre teste e treino, podemos usar:\n\nRPython\n\n\nlibrary(mlr3verse)\n\n# Seleciona a tarefa e o modelo\ntsk <- as_task_regr(humidity ~ maximum_temprature + wind_speed, data = dados)\nlearner <- lrn(\"regr.lm\")\n\n# Define estrategia de separacao da amostra\nresampling <- rsmp(\"holdout\", ratio = 0.7)\n\n# Treina o modelo\nresultados <- resample(tsk, learner, resampling)\n\n# Avalia predicoes\nmeasure <- msr(c(\"regr.mse\")) # MSE\nresultados$score(measure, ids = FALSE, predict_sets = \"test\")\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Seleciona preditores e Y\nX = c[['wind_speed', 'maximum_temprature']]\nY = c.humidity\n\n# Holdout\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n\n# Treina o modelo e checa erros\nreg = LinearRegression().fit(X_train, Y_train)\npred = reg.predict(X_test)\nmean_squared_error(Y_test, pred) # MSE\n\n\n\n\n\nTreine modelos lineares (OLS) usando a combinação de variáveis que você achar melhor.\n\n\n\nUsando o modelo treinado anterioremente, calcule diferentes métricas de validação.\n\n\n\nCrie uma função que rode esse workflow inteiro e retorne apenas uma métrica de validação. Rode essa função 100 vezes e reporte os resultados (como quiser, com gráfico ou outros).\n\n\n\nUsando a função anterior, teste diferentes combinações de variáveis no modelo para achar um que tenha uma boa performance."
  },
  {
    "objectID": "exercicios/exercicios4.html#a-regularização",
    "href": "exercicios/exercicios4.html#a-regularização",
    "title": "Exercícios 4",
    "section": "a) Regularização",
    "text": "a) Regularização\nUsando a mesma base de dados, adapte seu workflow anterior para, em vez de usar regressão linear, estimar modelos LASSO e Ridge."
  },
  {
    "objectID": "exercicios/exercicios4.html#b-funções",
    "href": "exercicios/exercicios4.html#b-funções",
    "title": "Exercícios 4",
    "section": "b) Funções",
    "text": "b) Funções\nCrie uma função para estimar LASSO e Ridge e compare os resultados de 100 execuções."
  },
  {
    "objectID": "exercicios/exercicios4.html#a-transformações-básicas",
    "href": "exercicios/exercicios4.html#a-transformações-básicas",
    "title": "Exercícios 4",
    "section": "a) Transformações básicas",
    "text": "a) Transformações básicas\nCrie uma nova variável que indique o percentual de votos válidos de Bolsonaro (dica: basta dividir votos_bolsonaro_2t_2018 por votos_validos_2t_2018)"
  },
  {
    "objectID": "exercicios/exercicios4.html#b-exploração",
    "href": "exercicios/exercicios4.html#b-exploração",
    "title": "Exercícios 4",
    "section": "b) Exploração",
    "text": "b) Exploração\nCrie alguns gráficos pra explorar a relação entre a votação de Bolsonaro e algumas das variáveis do banco (faça como quiser, e quantos gráficos quiser)."
  },
  {
    "objectID": "exercicios/exercicios4.html#c-modelos",
    "href": "exercicios/exercicios4.html#c-modelos",
    "title": "Exercícios 4",
    "section": "c) Modelos",
    "text": "c) Modelos\nRode modelos lineares, com e sem regularização, para tentar predizer a votação de Bolsonaro nos municípios usando variáveis como regiao, semiarido, capital, pib_total."
  },
  {
    "objectID": "exercicios/exercicios4.html#d-transformações",
    "href": "exercicios/exercicios4.html#d-transformações",
    "title": "Exercícios 4",
    "section": "d) Transformações",
    "text": "d) Transformações\nTransforme a variável pib_total para que ela fique estandardizada (vale ser criativo e explorar outras variáveis do banco)."
  },
  {
    "objectID": "exercicios/exercicios5.html",
    "href": "exercicios/exercicios5.html",
    "title": "Exercícios 5",
    "section": "",
    "text": "2) Evitando data leaks\nComo vimos, pré-processamento deve ser aplicado antes de fazermos split sample de validação (i.e., criar amostras de teste e de treino). Agora, implemente um workflow que leva isso em conta. Para tanto, você deverá criar uma função que separe textos em treino e teste, que aplique pré-processamento apenas na amostra de treino e que, depois, replique ele na amostra de teste para, então, rodar um algoritmo e calcular alguma métrica de validação.\n\n\n3) Benchmark\nUsando as ferramentas que vimos, experimente com os seguintes pré-processamentos:\n\n\nUsando apenas palavras maiores do que 4 caracteres;\n\n\nRemovendo palavras que não ocorrem em, pelo menos, 10 documentos;\n\n\nRemovendo palavras que não ocorrem em, pelo menos, 10% dos documentos;\n\n\nUsando TF-IDF para normalizar os elementos da matriz bag of words;"
  },
  {
    "objectID": "exercicios/exercicios6.html",
    "href": "exercicios/exercicios6.html",
    "title": "Exercícios 6",
    "section": "",
    "text": "Para esse exercício, precisaremos carregar dados climáticos de São Bernardo do Campo:\n\nRPython\n\n\nlink <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv\"\ndados <- readr::read_csv(link)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv'\ndados = pd.read_csv(link)\n\n\n\n\n\nUsando pipelines, crie três diferentes pré-processamentos para as features numéricas da base: a) uma sem transformações; b) outra fazendo estandardização das variáveis; e, c), outra incluindo alguns polinômios. As pipelines devem usar regressão linear simples como modelo para predizer a variável maximum_temprature.\n\n\n\nCompare as pipelines anteriores rodando 100 vezes cada uma usando holdout com 70% das observações em treino, calculando para cada também o RMSE. Reporte os resultados por meio de um gráfico de boxplot. Dica: use uma função para encapsular pipelines, treino dos modelos e cálculo de métricas de validação.\n\n\n\nSelecione a melhor pipeline do exercício anterior e crie outras três novas em cima dela: uma que regressão por knn em vez de regressão linear; uma que use MARS (o algoritmo earth); e, por fim, uma que use regressão por meio de árvore de decisão (tree ou regr.rpart). Rode 100 vezes cada pipeline e compare novamente os RMSE usando um gráfico de boxplot.\n\n\n\nUsando a melhor pipeline encontrada no exercício anterior, faça validação nas seguintes bases de dados:\n\nRPython\n\n\n# Clima em Campinas\ncampinas <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv\"\ncampinas <- readr::read_csv(campinas)\n\n# Clima em Southampton\nsouthampton <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/master/web%20scraping/Cleaned%20Data/United%20Kingdom_Southampton_Cleaned.csv\"\nsouthampton <- readr::read_csv(southampton)\n\n\n# Clima em Campinas\ncampinas = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv'\ncampinas = pd.read_csv(campinas)\n\n# Clima em Southampton\nsouthampton = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/master/web%20scraping/Cleaned%20Data/United%20Kingdom_Southampton_Cleaned.csv'\nsouthampton = pd.read_csv(southampton)\n\n\n\n\n\n\nUsando os resultados da melhor pipeline, plote a relação entre predições e valores reais de maximum_temprature nas duas bases de validação."
  },
  {
    "objectID": "exercicios/exercicios6.html#a-pipelines",
    "href": "exercicios/exercicios6.html#a-pipelines",
    "title": "Exercícios 6",
    "section": "a) Pipelines",
    "text": "a) Pipelines\nUsando pipelines, crie duas pipelines diferentes de pré-processamentos para as os discursos da base: a) uma que só mantenha termos que aparecem em pelo menos 20% dos documentos (ou ao menos em 20 documentos); outra igual a anterior que permita bi-gramas. As pipelines devem usar Naive Bayes como modelo para predizer a variável planalto."
  },
  {
    "objectID": "exercicios/exercicios6.html#b-benchmark-1",
    "href": "exercicios/exercicios6.html#b-benchmark-1",
    "title": "Exercícios 6",
    "section": "b) Benchmark",
    "text": "b) Benchmark\nRode cada pipeline 10 vezes, calculando o F1 de cada predição do modelo na base de teste que tenha 20% dos discursos. Plote os resultados usando boxplot."
  },
  {
    "objectID": "exercicios/exercicios6.html#c-modelos",
    "href": "exercicios/exercicios6.html#c-modelos",
    "title": "Exercícios 6",
    "section": "c) Modelos",
    "text": "c) Modelos\nUse a melhor pipeline para criar outra, que em vez de Naive Bayes use árvore de decisão (classif.rpart, no caso do mlr3). Rode 10 vezes cada uma, calcule e reporte o F1 para cada uma."
  },
  {
    "objectID": "exercicios/exercicios7.html",
    "href": "exercicios/exercicios7.html",
    "title": "Exercícios 7",
    "section": "",
    "text": "Para esse exercício, precisaremos de novos dados, dessa vez das eleições municípais de 2000. A base que usaremos indica qual partido venceu, se PMDB/PSDB/PFL ou outros, e variáveis econômicas e demográficas (não se esqueça de remover IDs e nome dos municípios, como cod_mun_ibge e nome_municipio; se usar Python, também não se esqueça de transformar/remover as variáveis uf e coligacao):11 Dica: se usar árvores de decisão, experimente aumentar o número delas.\n\nRPython\n\n\nlibrary(tidyverse)\n\nlink <- \"https://raw.githubusercontent.com/FLS-6497/datasets/main/aula7/eleicoes2000.csv\"\ndados <- readr::read_csv2(link) %>%\n  select(-cod_mun_ibge, -nome_municipio) %>%\n  mutate_if(is.character, as.factor)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/FLS-6497/datasets/main/aula7/eleicoes2000.csv'\ndados = pd.read_csv(link, sep=';', decimal=\",\").drop(['cod_mun_ibge', 'nome_municipio'], axis=1)\n\n\n\n\n\nExplore rapidamente a base de dados. Para tanto, você pode criar gráficos com as distribuições do target e de algumas features, com cruzamentos das variáveis ou, ainda, usar correlações. Quais variáveis parecem ter mais relação com o target partido?\n\n\n\nUsando pipelines, crie um bagging ensemble combinando quantos e quais modelos você quiser e outra pipeline usando Naive Bayes. Treine e compare os resultados destas pipelines.\n\n\n\nAgora, crie outros dois bagging ensembles, um deles fazendo subsample dos dados (no mlr3, isso é controlado pelo argumento frac no po com subsample) e, o outro, utilizando um modelo diferente do que você utilizou na bagg anterior. Treine e compare os resultados destas novas pipelines.\n\n\n\nCrie uma pipeline agora usando random forest (fique à vontade para customizar ela como achar melhor) e compare seus resultados com o da melhor pipeline que você encontrou no exercício anterior."
  },
  {
    "objectID": "exercicios/exercicios7.html#a-básico",
    "href": "exercicios/exercicios7.html#a-básico",
    "title": "Exercícios 7",
    "section": "a) Básico",
    "text": "a) Básico\nAdaptando o exemplo dos materiais de aula, crie uma pipeline que use stacking para combinar os resultados de três modelos diferentes. Os modelos de nível 0 podem ter tanto etapas de pré-processamento, modelos ou parâmetros diferentes (e.g., é possível treinar 3 árvores diferentes). Como blender, use um modelo de regressão logística simples (no mlr3, classif.log_ref; no sklearn, LogisticRegression). Treine e veja os resultados desta pipeline."
  },
  {
    "objectID": "exercicios/exercicios7.html#b-ensemble-em-cima-de-ensemble",
    "href": "exercicios/exercicios7.html#b-ensemble-em-cima-de-ensemble",
    "title": "Exercícios 7",
    "section": "b) Ensemble em cima de ensemble",
    "text": "b) Ensemble em cima de ensemble\nAo stack anterior, adapte e adicione agora o melhor bagging ensemble que você encontrou no exercício 1. Treine e veja o resultado dessa nova versão."
  },
  {
    "objectID": "exercicios/exercicios7.html#a-gradiente",
    "href": "exercicios/exercicios7.html#a-gradiente",
    "title": "Exercícios 7",
    "section": "a) Gradiente",
    "text": "a) Gradiente\nTreine dois ensembles com boosting, um usando gradient boosting e, o outro, extreme gradient boosting. Compare os resultados."
  },
  {
    "objectID": "exercicios/exercicios7.html#b-número-de-árvores-em-boosting",
    "href": "exercicios/exercicios7.html#b-número-de-árvores-em-boosting",
    "title": "Exercícios 7",
    "section": "b) Número de árvores em boosting",
    "text": "b) Número de árvores em boosting\nUsando extreme boosting, crie três pipelines: uma que treine 10 modelos, outra que treine 100 e, por fim, uma que treine 200. O que acontece com os resultados?"
  },
  {
    "objectID": "exercicios/exercicios8.html",
    "href": "exercicios/exercicios8.html",
    "title": "Exercícios 8",
    "section": "",
    "text": "Para esse exercício, usaremos novamente dados das eleições municipais de 2000:\n\nRPython\n\n\nlink <- \"https://raw.githubusercontent.com/FLS-6497/datasets/main/aula7/eleicoes2000.csv\"\ndados <- readr::read_csv2(link) %>%\n  select(-cod_mun_ibge, -nome_municipio) %>%\n  mutate_if(is.character, as.factor)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/FLS-6497/datasets/main/aula7/eleicoes2000.csv'\ndados = pd.read_csv(link, sep=';', decimal=\",\").drop(['cod_mun_ibge', 'nome_municipio'], axis=1)\n\n\n\n\n\nCom os dados, implemente uma pipeline de classificação que use PCA para reduzir o número de features nos dados. Seu objetivo será predizer qual partido governa dado município. Calcule alguma métrica de validação.\n\n\n\nPartindo da pipeline anterior, crie diferentes pipelines alterando o número de dimensões no PCA para 2, 3, 4 e 5. Rode essas pipelines e compare seus resultados.\n\n\n\nChecando a documentação do seu framework, implemente alguma alternativa ao PCA (exemplo: kernel PCA)."
  },
  {
    "objectID": "exercicios/exercicios8.html#a-k-means",
    "href": "exercicios/exercicios8.html#a-k-means",
    "title": "Exercícios 8",
    "section": "a) K-means",
    "text": "a) K-means\nUse K-means para separar os e as parlamentares em 4 grupos. Adicione essa classificação na base de dados original e a explore para tentar compreender quais são esses grupos."
  },
  {
    "objectID": "exercicios/exercicios8.html#b-alternativas",
    "href": "exercicios/exercicios8.html#b-alternativas",
    "title": "Exercícios 8",
    "section": "b) Alternativas",
    "text": "b) Alternativas\nUse outro algoritmo de clustering e faça uma nova classificação dos e das parlamentares. Compare com a anterior para examinar as diferenças."
  },
  {
    "objectID": "exercicios/exercicios9.html",
    "href": "exercicios/exercicios9.html",
    "title": "Exercícios 9",
    "section": "",
    "text": "Para esse exercício, usaremos uma base de dados das candidaturas à Câmara dos Deputados em 2014 que contém, entre outros, variáveis como o sexo, a raça, a escolaridade e o status de reeleição das candidaturas, bem como uma dummy (resultado) que indica se a candidatura foi (\\(1\\)) ou não (\\(0\\)) eleita (Machado, Campos, e Recch 2020).\n\nRPython\n\n\nlink <- \"https://raw.githubusercontent.com/FLS-6497/datasets/main/aula9/camara_2014.csv\"\ndados <- readr::read_csv2(link) %>%\n  mutate_if(is.character, as.factor)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/FLS-6497/datasets/main/aula9/camara_2014.csv'\ndados = pd.read_csv(link, sep=';', decimal=\",\")\n\n\n\n\n\nCrie uma pipeline para estandardizar variáveis numéricas (ou transformar variáveis categóricas em dummies) com algum modelo de classificação da sua escolha e o valide usando K-fold com \\(K = 5\\) e, depois, com \\(K = 10\\).\n\n\n\nSorteie apenas algumas observações do banco completo (50, por exemplo) e, em vez de usar K-fold, desta vez use LOO como estratégia de validação (no mlr3, a função chama-se loo; no sklearn, LeaveOneOut).11 No R talvez seja necessário usar como métrica de validação a classif.ce.\n\n\n\nNa base de dados, há muito menos candidaturas eleitas do que não-eleitas. Para evitar que amostras de treino e de teste percam esse balanço original, use K-fold estratificado (no mlr3, basta declarar stratum = variavel na task; no sklearn, use StratifiedKFold).\n\n\n\nFinalmente, use repeated k-fold para minimizar a variação decorrente do sorteio no particionamento das amostras (no mlr3, com repeated_cv; no sklearn, com RepeatedKFold ou com RepeatedStratifiedKFold)."
  },
  {
    "objectID": "exercicios/exercicios9.html#a-holdout",
    "href": "exercicios/exercicios9.html#a-holdout",
    "title": "Exercícios 9",
    "section": "a) Holdout",
    "text": "a) Holdout\nFaça um holdout inicial da base, separando 90% dela para treino e teste e 10% para validação."
  },
  {
    "objectID": "exercicios/exercicios9.html#b-cross-validation",
    "href": "exercicios/exercicios9.html#b-cross-validation",
    "title": "Exercícios 9",
    "section": "b) Cross-validation",
    "text": "b) Cross-validation\nCom os 90% restanted da base, treine e valide um modelo usando alguma estratégia de cross-validation. Ao final, quando encontrar o melhor modelo, treine ele em todos os 90% das observações e o valide na base de validação com 10% de observações."
  },
  {
    "objectID": "exercicios/exercicios9.html#a-novo-workflow",
    "href": "exercicios/exercicios9.html#a-novo-workflow",
    "title": "Exercícios 9",
    "section": "a) Novo workflow",
    "text": "a) Novo workflow\nMonte um workflow para melhorar o desempenho na tarefa de predizer maximum_temprature. Em particular, considere o seguinte:\n\nPré-processar variáveis contínuas (minmax ou estandardização);\nReduzir dimensionalidade (PCA ou kernelpca);\nConsiderar combinações não-lineares (criando polinômios ou usando MARS)\nUsar ensemble, inclusive com stacking\nUsar uma estratégia de validação que deixe mais dados para treino (K-fold com um \\(K=10\\) ou \\(K=20\\))\nConsiderar a estrutura temporal dos dados (é possível criar uma variável lag de maximum_temprature, o transformar o problema em um de série temporal e usar walk-forward validation)"
  },
  {
    "objectID": "exercicios/exercicios10.html",
    "href": "exercicios/exercicios10.html",
    "title": "Exercícios 10",
    "section": "",
    "text": "2) Tuning com text as data\nNeste exercício revisitaremos os dados do Projeto 1 para aplicar tuning às pipelines que vocês já montaram anteriormente (é possível ir no GitHub consultar seu código). Particularmente, tuning será útil para identificar melhores combinações de hiper-parâmetros de pré-processamento – número ou proporção mínima de ocorrência de palavras, número mínimo de ocorrência de uma palavra entre documentos, tamanho do \\(N\\) em \\(N-grams\\), etc.\nCarregue os dados com:\n\nRPython\n\n\nlink <- \"https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais.csv?raw=true\"\ndiscursos <- readr::read_csv2(link)\n\n\nimport pandas as pd\n\nlink = 'https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais.csv?raw=true'\ndiscursos = pd.read_csv(link, sep=';')\n\n\n\n\n\n3) Melhorando as predições climáticas\nNeste exercício final, usaremos tuning para dar um passo adicional na tarefa de predizer a temparatura máxima diária em São Bernardo do Campo (SP). Para isso, use seu código da última aula e o adapte para fazer tuning de hiper-parâmetros (é possível usar o dicionário do mlr3 já com combinações prontas de hiper-parâmetros).\nCarregue os dados com:\n\nRPython\n\n\nlink <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv\"\ndados <- readr::read_csv(link)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv'\ndados = pd.read_csv(link)\n\n\n\nAo final, valide a sua melhor pipeline com dados de Campinas:\n\nRPython\n\n\ncampinas <- \"https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Campinas_Cleaned.csv\"\ncampinas <- readr::read_csv(campinas)\n\n\nimport pandas as pd\n\nlink = 'https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Campinas_Cleaned.csv'\ncampinas = pd.read_csv(campinas)\n\n\n\n\n\n\n\n\n\nReferências\n\nKaufman, Aaron Russell, Peter Kraft, e Maya Sen. 2019. «Improving supreme court forecasting using boosted decision trees». Political Analysis 27 (3): 381–87."
  },
  {
    "objectID": "exercicios/exercicios11.html",
    "href": "exercicios/exercicios11.html",
    "title": "Exercícios 11",
    "section": "",
    "text": "2) Rede neural convolucional\nAgora, adaptando o código que vimos para um modelo neural convolucional, repita o esforço do exercício 1 e tente melhorar o seu desempenho na base de validação."
  }
]